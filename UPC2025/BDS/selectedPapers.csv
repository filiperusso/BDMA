contentType,identifier,language,url,title,creators,publicationName,doi,publisher,publisherName,publicationDate,publicationType,issn,eIssn,volume,number,issueType,topicalCollection,genre,startingPage,endingPage,journalId,openAccess,onlineDate,coverDate,copyright,subjects,disciplines,key_issn,h_index,abstract.h1,abstract.p,printDate,monthYear
Article,doi:10.1038/s41598-025-05026-9,en,"[{'format': '', 'platform': '', 'value': 'http://dx.doi.org/10.1038/s41598-025-05026-9'}]",The usage of a transformer based and artificial intelligence driven multidimensional feedback system in english writing instruction,"[{'creator': 'Zheng, Xiaofeng'}, {'creator': 'Zhang, Jian'}]",Scientific Reports,10.1038/s41598-025-05026-9,Nature,Nature Publishing Group UK,2025-06-02,Journal,,2045-2322,15,1,Regular,,"['OriginalPaper', 'Article']",1,22,41598,true,2025-06-02,2025-12,©2025 The Author(s),"['Science, Humanities and Social Sciences, multidisciplinary', 'Science, Humanities and Social Sciences, multidisciplinary', 'Science, multidisciplinary']","[{'id': '2887', 'term': 'Technology and Engineering'}, {'id': '2867', 'term': 'Physical Sciences'}, {'id': '2874', 'term': 'Life Sciences'}, {'id': '3001', 'term': 'Behavioral Sciences and Psychology'}, {'id': '2971', 'term': 'Computer Science'}]",2045-2322,347,Abstract,"The need for personalized and real-time feedback in English writing instruction is increasing rapidly. Traditional systems, which depend on rule-based engines and shallow machine learning models, struggle to meet this demand. They often fall short in addressing key aspects such as grammar correction, sentence variety, and logical coherence. This study introduces a multidimensional feedback system based on the Transformer architecture. The system combines self-attention mechanisms with a dynamic parameter adjustment module to deliver feedback at multiple levels—from individual words to entire paragraphs. A BERT model is fine-tuned on a large, diverse corpus that includes academic papers, blog posts, and student essays. As a result, the system can provide real-time suggestions that address grammar, vocabulary, sentence structure, and logic. Experimental results show that the system improves the writing quality of non-native learners while maintaining a feedback delay of just 1.8 s. Its modular design allows for the customization of learning paths, and user privacy is protected through differential privacy mechanisms. This approach offers a technically sound and educationally practical solution for developing AI-assisted writing tools across disciplines.",,Jun25
Article,doi:10.1007/s00521-025-11145-1,en,"[{'format': '', 'platform': '', 'value': 'http://dx.doi.org/10.1007/s00521-025-11145-1'}]",Transformers to the rescue: alleviating data scarcity in arabic grammatical error correction with pre-trained models,"[{'creator': 'Ismail, Karim'}, {'creator': 'Abdou, Sherif'}, {'creator': 'Farouk, Mohamed'}, {'ORCID': '0000-0002-0456-2276', 'creator': 'Salem, Ahmed'}]",Neural Computing and Applications,10.1007/s00521-025-11145-1,Springer,Springer London,2025-06-01,Journal,0941-0643,1433-3058,37,18,Regular,,"['OriginalPaper', 'Original Article']",13011,13038,521,true,2025-04-24,2025-06,©2025 The Author(s),"['Computer Science', 'Artificial Intelligence', 'Data Mining and Knowledge Discovery', 'Probability and Statistics in Computer Science', 'Computational Science and Engineering', 'Image Processing and Computer Vision', 'Computational Biology/Bioinformatics']","[{'id': '2970', 'term': 'Artificial Intelligence'}, {'id': '3820', 'term': 'Data Mining and Knowledge Discovery'}, {'id': '5673', 'term': 'Probability and Statistics in Computer Science'}, {'id': '4149', 'term': 'Computational Science and Engineering'}, {'id': '4777', 'term': 'Computer Vision'}, {'id': '2912', 'term': 'Computational and Systems Biology'}]",0941-0643,146,Abstract,"Grammatical error correction (GEC) in Arabic presents unique challenges arising from complex morphology and contextual intricacies. Current methodologies predominantly rely on neural machine translation (NMT) models, hindered by adequately annotated training data scarcity. This research introduces a novel approach utilizing pre-trained transformers, specifically sequence-to-sequence (seq2seq) models, such as AraT5 and AraBART, alongside their multilingual variants (mT5 and mBART), to address Arabic GEC. These transformers, initially designed for diverse natural language processing tasks, demonstrate promising results in GEC, particularly when parallel data are limited. Employing tokenization and preprocessing techniques on publicly accessible GEC datasets, we train the transformers using a supervised approach. The experimental results showcase superior performance, surpassing previous models with an F1 score of 92.1% on the QALB 2014 dataset, 89.4% on the QALB 2015 native test data, and 83.6% on non-native data. This highlights the effectiveness of the proposed methodology in rectifying various grammatical errors in Arabic text. In conclusion, this study contributes to advancing the field of Arabic GEC by leveraging transfer learning with pre-trained transformers. The findings underscore the potential of this approach to overcome challenges posed by limited data availability, with AraBART emerging as a practical choice. This research opens avenues for further exploration in low-resource languages. It suggests potential applications in high-resource languages, encouraging future comparative studies.",2025-06-17,Apr25
Article,doi:10.1038/s41598-025-99515-6,en,"[{'format': '', 'platform': '', 'value': 'http://dx.doi.org/10.1038/s41598-025-99515-6'}]",Emotion-Aware RoBERTa enhanced with emotion-specific attention and TF-IDF gating for fine-grained emotion recognition,"[{'creator': 'Alqarni, Fatimah'}, {'creator': 'Sagheer, Alaa'}, {'creator': 'Alabbad, Amira'}, {'creator': 'Hamdoun, Hala'}]",Scientific Reports,10.1038/s41598-025-99515-6,Nature,Nature Publishing Group UK,2025-05-21,Journal,,2045-2322,15,1,Regular,,"['OriginalPaper', 'Article']",1,19,41598,true,2025-05-21,2025-12,©2025 The Author(s),"['Science, Humanities and Social Sciences, multidisciplinary', 'Science, Humanities and Social Sciences, multidisciplinary', 'Science, multidisciplinary']","[{'id': '2887', 'term': 'Technology and Engineering'}, {'id': '2867', 'term': 'Physical Sciences'}, {'id': '2874', 'term': 'Life Sciences'}, {'id': '3001', 'term': 'Behavioral Sciences and Psychology'}, {'id': '2971', 'term': 'Computer Science'}]",2045-2322,347,Abstract,"Emotion recognition in text is a fundamental task in natural language processing, underpinning applications such as sentiment analysis, mental health monitoring, and content moderation. Although transformer-based models like RoBERTa have advanced contextual understanding in text, they still face limitations in identifying subtle emotional cues, handling class imbalances, and processing noisy or informal input. To address these challenges, this paper introduces Emotion-Aware RoBERTa, an enhanced framework that integrates an Emotion-Specific Attention (ESA) layer and a TF-IDF based gating mechanism. These additions are designed to dynamically prioritize emotionally salient tokens while suppressing irrelevant content, thereby improving both classification accuracy and robustness. The model achieved 96.77% accuracy and a weighted F1-score of 0.97 on the primary dataset, outperforming baseline RoBERTa and other benchmark models such as DistilBERT and ALBERT with a relative improvement ranging from 9.68% to 10.87%. Its generalization capability was confirmed across two external datasets, achieving 88.03% on a large-scale corpus and 65.67% on a smaller, noisier dataset. An ablation study revealed the complementary impact of the ESA and TF-IDF components, balancing performance and inference efficiency. Attention heatmaps were used to visualize ESA’s ability to focus on key emotional expressions, while inference-time optimizations using FP16 and Automatic Mixed Precision (AMP) reduced memory consumption and latency. Additionally, McNemar’s statistical test confirmed the significance of the improvements over the baseline. These findings demonstrate that Emotion-Aware RoBERTa offers a scalable, interpretable, and deployment-friendly solution for fine-grained emotion recognition, making it well-suited for real-world NLP applications in emotion-aware systems.",,May25
Article,doi:10.1038/s41598-025-97500-7,en,"[{'format': '', 'platform': '', 'value': 'http://dx.doi.org/10.1038/s41598-025-97500-7'}]",Cross language transformation of free text into structured lobectomy surgical records from a multi center study,"[{'creator': 'Yang, Xiongwen'}, {'creator': 'Xiao, Yi'}, {'creator': 'Liu, Di'}, {'creator': 'Deng, Huiyin'}, {'creator': 'Huang, Jian'}, {'creator': 'Zhou, Yubin'}, {'creator': 'Dai, Chuanzhou'}, {'creator': 'Wu, Jun'}, {'creator': 'Liu, Dan'}, {'creator': 'Liang, Maoli'}, {'creator': 'Xu, Chuan'}]",Scientific Reports,10.1038/s41598-025-97500-7,Nature,Nature Publishing Group UK,2025-05-02,Journal,,2045-2322,15,1,Regular,,"['OriginalPaper', 'Article']",1,11,41598,true,2025-05-02,2025-12,©2025 The Author(s),"['Science, Humanities and Social Sciences, multidisciplinary', 'Science, Humanities and Social Sciences, multidisciplinary', 'Science, multidisciplinary']","[{'id': '2887', 'term': 'Technology and Engineering'}, {'id': '2867', 'term': 'Physical Sciences'}, {'id': '2874', 'term': 'Life Sciences'}, {'id': '3001', 'term': 'Behavioral Sciences and Psychology'}, {'id': '2971', 'term': 'Computer Science'}]",2045-2322,347,Abstract,"In a recent study, the effectiveness of GPT-4 Omni in transforming lobectomy surgical records into structured data across multiple languages was explored. The aim was to improve both efficiency and accuracy in documenting thoracic surgical oncology procedures. Involving 466 records from seven specialized hospitals, the process started with OCR and text normalization. A manual restructuring by thoracic oncologists set the benchmark for fine-tuning Generative Pre-trained Transformer 4 Omni (GPT-4o). Experts reviewed the AI’s output, assessing it on accuracy, precision, recall, and F1 scores. GPT-4o demonstrated high performance across both Chinese and English records, achieving an accuracy of 0.966, precision of 0.981, recall of 0.982, and an F1-score of 0.982 in both language settings. Results showed that GPT-4o was highly effective in both Chinese and English, significantly speeding up documentation compared to traditional methods. While it performed well across languages and reduced review times, common error types included terminology misinterpretations (2.82%), procedural sequence errors (1.41%), and omissions of key details (0.47%). While it performed well across languages and reduced review times, these limitations highlight areas for further refinement, particularly in enhancing contextual understanding and mitigating minor errors. Nonetheless, GPT-4o shows great potential in standardizing surgical records, streamlining workflows, and boosting care and research in thoracic oncology.",,May25
Article,doi:10.1038/s41598-025-87862-3,en,"[{'format': '', 'platform': '', 'value': 'http://dx.doi.org/10.1038/s41598-025-87862-3'}]",An LLM-based hybrid approach for enhanced automated essay scoring,"[{'creator': 'Atkinson, John'}, {'creator': 'Palma, Diego'}]",Scientific Reports,10.1038/s41598-025-87862-3,Nature,Nature Publishing Group UK,2025-04-25,Journal,,2045-2322,15,1,Regular,,"['OriginalPaper', 'Article']",1,9,41598,true,2025-04-25,2025-12,©2025 The Author(s),"['Science, Humanities and Social Sciences, multidisciplinary', 'Science, Humanities and Social Sciences, multidisciplinary', 'Science, multidisciplinary']","[{'id': '2887', 'term': 'Technology and Engineering'}, {'id': '2867', 'term': 'Physical Sciences'}, {'id': '2874', 'term': 'Life Sciences'}, {'id': '3001', 'term': 'Behavioral Sciences and Psychology'}, {'id': '2971', 'term': 'Computer Science'}]",2045-2322,347,Abstract,"Automated Essay Scoring systems have traditionally relied on shallow lexical data, such as word frequency and sentence length, to assess essays. However, these approaches neglect crucial factors like text structure and semantics, resulting in limited evaluations of coherence and quality. To address these limitations, we propose a hybrid approach to AES that combines multiple features from different linguistic levels. By leveraging the complementary nature of these features, our model captures the intricate relationships underlying coherent texts. Through extensive experimentation using standard essay datasets, we demonstrate that our large language model based hybrid model surpasses state-of-the-art methods based on shallow features and pure neural networks. This research represents a significant advancement towards the development of an accurate and effective tool for assessing student writing.",,Apr25
Article,doi:10.1038/s41598-025-98483-1,en,"[{'format': '', 'platform': '', 'value': 'http://dx.doi.org/10.1038/s41598-025-98483-1'}]",Industrial applications of large language models,"[{'creator': 'Raza, Mubashar'}, {'creator': 'Jahangir, Zarmina'}, {'creator': 'Riaz, Muhammad Bilal'}, {'creator': 'Saeed, Muhammad Jasim'}, {'creator': 'Sattar, Muhammad Awais'}]",Scientific Reports,10.1038/s41598-025-98483-1,Nature,Nature Publishing Group UK,2025-04-21,Journal,,2045-2322,15,1,Regular,,"['OriginalPaper', 'Article']",1,23,41598,true,2025-04-21,2025-12,©2025 The Author(s),"['Science, Humanities and Social Sciences, multidisciplinary', 'Science, Humanities and Social Sciences, multidisciplinary', 'Science, multidisciplinary']","[{'id': '2887', 'term': 'Technology and Engineering'}, {'id': '2867', 'term': 'Physical Sciences'}, {'id': '2874', 'term': 'Life Sciences'}, {'id': '3001', 'term': 'Behavioral Sciences and Psychology'}, {'id': '2971', 'term': 'Computer Science'}]",2045-2322,347,Abstract,"Large language models (LLMs) are artificial intelligence (AI) based computational models designed to understand and generate human like text. With billions of training parameters, LLMs excel in identifying intricate language patterns, enabling remarkable performance across a variety of natural language processing (NLP) tasks. After the introduction of transformer architectures, they are impacting the industry with their text generation capabilities. LLMs play an innovative role across various industries by automating NLP tasks. In healthcare, they assist in diagnosing diseases, personalizing treatment plans, and managing patient data. LLMs provide predictive maintenance in automotive industry. LLMs provide recommendation systems, and consumer behavior analyzers. LLMs facilitates researchers and offer personalized learning experiences in education. In finance and banking, LLMs are used for fraud detection, customer service automation, and risk management. LLMs are driving significant advancements across the industries by automating tasks, improving accuracy, and providing deeper insights. Despite these advancements, LLMs face challenges such as ethical concerns, biases in training data, and significant computational resource requirements, which must be addressed to ensure impartial and sustainable deployment. This study provides a comprehensive analysis of LLMs, their evolution, and their diverse applications across industries, offering researchers valuable insights into their transformative potential and the accompanying limitations.",,Apr25
Article,doi:10.1007/s00330-025-11500-9,en,"[{'format': '', 'platform': '', 'value': 'http://dx.doi.org/10.1007/s00330-025-11500-9'}]",Impact of hospital-specific domain adaptation on BERT-based models to classify neuroradiology reports,"[{'creator': 'Agarwal, Siddharth'}, {'creator': 'Wood, David'}, {'creator': 'Murray, Benjamin A. K.'}, {'creator': 'Wei, Yiran'}, {'creator': 'Busaidi, Ayisha Al'}, {'creator': 'Kafiabadi, Sina'}, {'creator': 'Guilhem, Emily'}, {'creator': 'Lynch, Jeremy'}, {'creator': 'Townend, Matthew'}, {'creator': 'Mazumder, Asif'}, {'creator': 'Barker, Gareth J.'}, {'creator': 'Cole, James H.'}, {'creator': 'Sasieni, Peter'}, {'creator': 'Ourselin, Sebastien'}, {'creator': 'Modat, Marc'}, {'ORCID': '0000-0003-0984-3998', 'creator': 'Booth, Thomas C.'}]",European Radiology,10.1007/s00330-025-11500-9,Springer,Springer Berlin Heidelberg,2025-03-17,Journal,,1432-1084,,,,,"['OriginalPaper', 'Imaging Informatics and Artificial Intelligence']",1,15,330,true,2025-03-17,,©2025 The Author(s),"['Medicine & Public Health', 'Imaging / Radiology', 'Diagnostic Radiology', 'Interventional Radiology', 'Neuroradiology', 'Ultrasound', 'Internal Medicine']","[{'id': '3270', 'term': 'Biological Imaging'}, {'id': '2963', 'term': 'Radiology'}, {'id': '5301', 'term': 'Interventional Radiology'}, {'id': '7122', 'term': 'Neuroradiology'}, {'id': '8169', 'term': 'Ultrasonics'}, {'id': '2992', 'term': 'Internal Medicine'}]",1432-1084,182,Abstract,"['Objectives', 'To determine the effectiveness of hospital-specific domain adaptation through masked language modelling (MLM) on BERT-based models’ performance in classifying neuroradiology reports, and to compare these models with open-source large language models (LLMs).', 'Materials and methods', 'This retrospective study (2008–2019) utilised 126,556 and 86,032 MRI brain reports from two tertiary hospitals—King’s College Hospital (KCH) and Guys and St Thomas’ Trust (GSTT). Various BERT-based models, including RoBERTa, BioBERT and RadBERT, underwent MLM on unlabelled reports from these centres. The downstream tasks were binary abnormality classification and multi-label classification. Performances of models with and without hospital-specific domain adaptation were compared against each other and LLMs on internal (KCH) and external (GSTT) hold-out test sets. Model performances for binary classification were compared using 2-way and 1-way ANOVA.', 'Results', 'All models that underwent hospital-specific domain adaptation performed better than their baseline counterparts (all p -values\u2009<\u20090.001). For binary classification, MLM on all available unlabelled reports (194,467 reports) yielded the highest balanced accuracies (KCH: mean 97.0\u2009±\u20090.4% (standard deviation), GSTT: 95.5\u2009±\u20091.0%), after which no differences between BERT-based models remained (1-way ANOVA, p -values\u2009>\u20090.05). There was a log-linear relationship between the number of reports and performance. LLama-3.0 70B was the best-performing LLM (KCH: 97.1%, GSTT: 94.0%). Multi-label classification demonstrated consistent performance improvements from MLM for all abnormality categories.', 'Conclusion', 'Hospital-specific domain adaptation should be considered best practice when deploying BERT-based models in new clinical settings. When labelled data is scarce or unavailable, LLMs can serve as a viable alternative, assuming adequate computational power is accessible.', 'Key Points', 'Question BERT-based models can classify radiology reports, but it is unclear if there is any incremental benefit from additional hospital-specific domain adaptation . Findings Hospital-specific domain adaptation resulted in the highest BERT-based model accuracies and performance scaled log-linearly with the number of reports . Clinical relevance BERT-based models after hospital-specific domain adaptation achieve the best classification results provided sufficient high-quality training labels. When labelled data is scarce, LLMs such as Llama-3.0 70B are a viable alternative provided there are sufficient computational resources .', 'Graphical Abstract', '']",,Mar25
Article,doi:10.1038/s41598-024-82192-2,en,"[{'format': '', 'platform': '', 'value': 'http://dx.doi.org/10.1038/s41598-024-82192-2'}]",GPT-4 shows potential for identifying social anxiety from clinical interview data,"[{'ORCID': '0009-0005-3344-4753', 'creator': 'Ohse, Julia'}, {'ORCID': '0009-0003-1197-7255', 'creator': 'Hadžić, Bakir'}, {'ORCID': '0009-0001-7448-7857', 'creator': 'Mohammed, Parvez'}, {'ORCID': '0009-0008-9481-9354', 'creator': 'Peperkorn, Nicolina'}, {'ORCID': '0009-0000-9854-3780', 'creator': 'Fox, Janosch'}, {'ORCID': '0009-0008-1242-9070', 'creator': 'Krutzki, Joshua'}, {'ORCID': '0009-0005-9843-8776', 'creator': 'Lyko, Alexander'}, {'ORCID': '0000-0002-0492-4708', 'creator': 'Mingyu, Fan'}, {'ORCID': '0000-0001-9947-2742', 'creator': 'Zheng, Xiaohu'}, {'ORCID': '0000-0002-8254-8293', 'creator': 'Rätsch, Matthias'}, {'ORCID': '0000-0002-6281-0901', 'creator': 'Shiban, Youssef'}]",Scientific Reports,10.1038/s41598-024-82192-2,Nature,Nature Publishing Group UK,2024-12-16,Journal,,2045-2322,14,1,Regular,,"['OriginalPaper', 'Article']",1,12,41598,true,2024-12-16,2024-12,©2024 The Author(s),"['Science, Humanities and Social Sciences, multidisciplinary', 'Science, Humanities and Social Sciences, multidisciplinary', 'Science, multidisciplinary']","[{'id': '2887', 'term': 'Technology and Engineering'}, {'id': '2867', 'term': 'Physical Sciences'}, {'id': '2874', 'term': 'Life Sciences'}, {'id': '3001', 'term': 'Behavioral Sciences and Psychology'}, {'id': '2971', 'term': 'Computer Science'}]",2045-2322,347,Abstract,"While the potential of Artificial Intelligence (AI)—particularly Natural Language Processing (NLP) models—for detecting symptoms of depression from text has been vastly researched, only a few studies examine such potential for the detection of social anxiety symptoms. We investigated the ability of the large language model (LLM) GPT-4 to correctly infer social anxiety symptom strength from transcripts obtained from semi-structured interviews. N  = 51 adult participants were recruited from a convenience sample of the German population. Participants filled in a self-report questionnaire on social anxiety symptoms (SPIN) prior to being interviewed on a secure online teleconference platform. Transcripts from these interviews were then evaluated by GPT-4. GPT-4 predictions were highly correlated ( r  = 0.79) with scores obtained on the social anxiety self-report measure. Following the cut-off conventions for this population, an F_1 accuracy score of 0.84 could be obtained. Future research should examine whether these findings hold true in larger and more diverse datasets.",,Dec24
Article,doi:10.1038/s41598-024-69664-1,en,"[{'format': '', 'platform': '', 'value': 'http://dx.doi.org/10.1038/s41598-024-69664-1'}]",Novel concept-based image captioning models using LSTM and multi-encoder transformer architecture,"[{'creator': 'Osman, Asmaa A. E.'}, {'creator': 'Shalaby, Mohamed A. Wahby'}, {'creator': 'Soliman, Mona M.'}, {'creator': 'Elsayed, Khaled M.'}]",Scientific Reports,10.1038/s41598-024-69664-1,Nature,Nature Publishing Group UK,2024-09-05,Journal,,2045-2322,14,1,Regular,,"['OriginalPaper', 'Article']",1,15,41598,true,2024-09-05,2024-12,©2024 The Author(s),"['Science, Humanities and Social Sciences, multidisciplinary', 'Science, Humanities and Social Sciences, multidisciplinary', 'Science, multidisciplinary']","[{'id': '2887', 'term': 'Technology and Engineering'}, {'id': '2867', 'term': 'Physical Sciences'}, {'id': '2874', 'term': 'Life Sciences'}, {'id': '3001', 'term': 'Behavioral Sciences and Psychology'}, {'id': '2971', 'term': 'Computer Science'}]",2045-2322,347,Abstract,"Captioning an image involves using a combination of vision and language models to describe the image in an expressive and concise sentence. Successful captioning task requires extracting as much information as possible from the corresponding image. One of these key pieces of information is the topic to which the image belongs. The state-of-the-art methods used topic modeling depending only on caption text in order to extract these topics. The problem with extracting the topics using topic modeling only on caption text is that it lacks the consideration of the image’s semantic information. Instead, concept modeling extracts the concepts directly from the images in addition to considering the corresponding caption text. Concept modeling can be used in image captioning to extremely capture the image contexts and benefit from it to produce more accurate descriptions. In this paper, novel image captioning models are proposed by utilizing the concept modeling technique. The first concept-based model is proposed by utilizing LSTM as a decoder while the second model is proposed in association with new multi-encoder transformer architecture. Standard metrics have been used to evaluate the proposed models using Microsoft COCO and Flickr30K datasets. The proposed models outperformed the related work methods with reduced computational complexity.",,Sep24
Article,doi:10.1186/s12859-024-05903-6,en,"[{'format': '', 'platform': '', 'value': 'http://dx.doi.org/10.1186/s12859-024-05903-6'}]",VAIV bio-discovery service using transformer model and retrieval augmented generation,"[{'creator': 'Kim, Seonho'}, {'creator': 'Yoon, Juntae'}]",BMC Bioinformatics,10.1186/s12859-024-05903-6,BioMed Central,BioMed Central,2024-08-21,Journal,,1471-2105,25,1,Regular,Big data management in biological domains,"['OriginalPaper', 'Database']",1,25,12859,true,2024-08-21,2024-12,©2024 The Author(s),"['Life Sciences', 'Bioinformatics', 'Microarrays', 'Computational Biology/Bioinformatics', 'Computer Appl. in Life Sciences', 'Algorithms']","[{'id': '7750', 'term': 'Bioinformatics'}, {'id': '2912', 'term': 'Computational and Systems Biology'}, {'id': '5024', 'term': 'Algorithms'}]",1471-2105,251,Abstract,"['Background', 'There has been a considerable advancement in AI technologies like LLM and machine learning to support biomedical knowledge discovery.', 'Main body', 'We propose a novel biomedical neural search service called ‘VAIV Bio-Discovery’, which supports enhanced knowledge discovery and document search on unstructured text such as PubMed. It mainly handles with information related to chemical compound/drugs, gene/proteins, diseases, and their interactions (chemical compounds/drugs-proteins/gene including drugs-targets, drug-drug, and drug-disease). To provide comprehensive knowledge, the system offers four search options: basic search, entity and interaction search, and natural language search. We employ T5slim_dec, which adapts the autoregressive generation task of the T5 (text-to-text transfer transformer) to the interaction extraction task by removing the self-attention layer in the decoder block. It also assists in interpreting research findings by summarizing the retrieved search results for a given natural language query with Retrieval Augmented Generation (RAG). The search engine is built with a hybrid method that combines neural search with the probabilistic search, BM25.', 'Conclusion', 'As a result, our system can better understand the context, semantics and relationships between terms within the document, enhancing search accuracy. This research contributes to the rapidly evolving biomedical field by introducing a new service to access and discover relevant knowledge.']",,Aug24

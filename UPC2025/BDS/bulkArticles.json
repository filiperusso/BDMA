{"records": [{"contentType": "Article", "identifier": "doi:10.1038/s41598-025-05026-9", "language": "en", "url": [{"format": "", "platform": "", "value": "http://dx.doi.org/10.1038/s41598-025-05026-9"}], "title": "The usage of a transformer based and artificial intelligence driven multidimensional feedback system in english writing instruction", "creators": [{"creator": "Zheng, Xiaofeng"}, {"creator": "Zhang, Jian"}], "publicationName": "Scientific Reports", "doi": "10.1038/s41598-025-05026-9", "publisher": "Nature", "publisherName": "Nature Publishing Group UK", "publicationDate": "2025-06-02", "publicationType": "Journal", "issn": "", "eIssn": "2045-2322", "volume": "15", "number": "1", "issueType": "Regular", "topicalCollection": "", "genre": ["OriginalPaper", "Article"], "startingPage": "1", "endingPage": "22", "journalId": "41598", "openAccess": "true", "onlineDate": "2025-06-02", "coverDate": "2025-12", "copyright": "\u00a92025 The Author(s)", "abstract": {"h1": "Abstract", "p": "The need for personalized and real-time feedback in English writing instruction is increasing rapidly. Traditional systems, which depend on rule-based engines and shallow machine learning models, struggle to meet this demand. They often fall short in addressing key aspects such as grammar correction, sentence variety, and logical coherence. This study introduces a multidimensional feedback system based on the Transformer architecture. The system combines self-attention mechanisms with a dynamic parameter adjustment module to deliver feedback at multiple levels\u2014from individual words to entire paragraphs. A BERT model is fine-tuned on a large, diverse corpus that includes academic papers, blog posts, and student essays. As a result, the system can provide real-time suggestions that address grammar, vocabulary, sentence structure, and logic. Experimental results show that the system improves the writing quality of non-native learners while maintaining a feedback delay of just 1.8\u00a0s. Its modular design allows for the customization of learning paths, and user privacy is protected through differential privacy mechanisms. This approach offers a technically sound and educationally practical solution for developing AI-assisted writing tools across disciplines."}, "subjects": ["Science, Humanities and Social Sciences, multidisciplinary", "Science, Humanities and Social Sciences, multidisciplinary", "Science, multidisciplinary"], "disciplines": [{"id": "2887", "term": "Technology and Engineering"}, {"id": "2867", "term": "Physical Sciences"}, {"id": "2874", "term": "Life Sciences"}, {"id": "3001", "term": "Behavioral Sciences and Psychology"}, {"id": "2971", "term": "Computer Science"}]}, {"contentType": "Article", "identifier": "doi:10.1007/s40692-024-00325-y", "language": "en", "url": [{"format": "", "platform": "", "value": "http://dx.doi.org/10.1007/s40692-024-00325-y"}], "title": "Unleashing the transformers: NLP models detect AI writing in education", "creators": [{"ORCID": "0000-0002-2599-5468", "creator": "Campino, Jos\u00e9"}], "publicationName": "Journal of Computers in Education", "doi": "10.1007/s40692-024-00325-y", "publisher": "Springer", "publisherName": "Springer Berlin Heidelberg", "publicationDate": "2025-06-01", "publicationType": "Journal", "issn": "2197-9987", "eIssn": "2197-9995", "volume": "12", "number": "2", "issueType": "Regular", "topicalCollection": "", "genre": "OriginalPaper", "startingPage": "645", "endingPage": "673", "journalId": "40692", "openAccess": "true", "printDate": "2025-05-28", "onlineDate": "2024-06-13", "coverDate": "2025-06", "copyright": "\u00a92024 The Author(s)", "abstract": {"h1": "Abstract", "p": "Artificial Intelligence (AI) has witnessed widespread application across diverse domains, with education being a prominent focus for enhancing learning outcomes and tailoring educational approaches. Transformer models, exemplified by BERT, have demonstrated remarkable efficacy in Natural Language Processing (NLP) tasks. This research scrutinizes the current landscape of AI in education, emphasizing the utilization of transformer models. Specifically, the research delves into the influence of AI tools facilitating text generation through input prompts, with a notable instance being the GPT-4 model developed by OpenAI. The study employs pre-trained transformer models to discern whether a given text originates from AI or human sources. Notably, BERT emerges as the most effective model, fine-tuned using a dataset comprising abstracts authored by humans and those generated by AI. The outcomes reveal a heightened accuracy in distinguishing AI-generated text. These findings bear significance for the educational realm, suggesting that while endorsing the use of such tools for learning, vigilance is warranted to identify potential misuse or instances where students should independently develop their reasoning skills. Nevertheless, ethical considerations must be paramount when employing such methodologies. We have highlighted vulnerabilities concerning the potential bias of AI models towards non-native English speakers, stemming from possible deficiencies in vocabulary and grammatical structure. Additionally, users must ensure that there is no complete reliance on these systems to ascertain students' performance. Further research is imperative to unleash the full potential of AI in education and address ethical considerations tied to its application."}, "subjects": ["Education", "Educational Technology", "Computers and Education"], "disciplines": [{"id": "8186", "term": "Digital Education and Educational Technology"}, {"id": "6060", "term": "Computers and Education"}]}, {"contentType": "Article", "identifier": "doi:10.1007/s00521-025-11145-1", "language": "en", "url": [{"format": "", "platform": "", "value": "http://dx.doi.org/10.1007/s00521-025-11145-1"}], "title": "Transformers to the rescue: alleviating data scarcity in arabic grammatical error correction with pre-trained models", "creators": [{"creator": "Ismail, Karim"}, {"creator": "Abdou, Sherif"}, {"creator": "Farouk, Mohamed"}, {"ORCID": "0000-0002-0456-2276", "creator": "Salem, Ahmed"}], "publicationName": "Neural Computing and Applications", "doi": "10.1007/s00521-025-11145-1", "publisher": "Springer", "publisherName": "Springer London", "publicationDate": "2025-06-01", "publicationType": "Journal", "issn": "0941-0643", "eIssn": "1433-3058", "volume": "37", "number": "18", "issueType": "Regular", "topicalCollection": "", "genre": ["OriginalPaper", "Original Article"], "startingPage": "13011", "endingPage": "13038", "journalId": "521", "openAccess": "true", "printDate": "2025-06-17", "onlineDate": "2025-04-24", "coverDate": "2025-06", "copyright": "\u00a92025 The Author(s)", "abstract": {"h1": "Abstract", "p": "Grammatical error correction (GEC) in Arabic presents unique challenges arising from complex morphology and contextual intricacies. Current methodologies predominantly rely on neural machine translation (NMT) models, hindered by adequately annotated training data scarcity. This research introduces a novel approach utilizing pre-trained transformers, specifically sequence-to-sequence (seq2seq) models, such as AraT5 and AraBART, alongside their multilingual variants (mT5 and mBART), to address Arabic GEC. These transformers, initially designed for diverse natural language processing tasks, demonstrate promising results in GEC, particularly when parallel data are limited. Employing tokenization and preprocessing techniques on publicly accessible GEC datasets, we train the transformers using a supervised approach. The experimental results showcase superior performance, surpassing previous models with an F1 score of 92.1% on the QALB 2014 dataset, 89.4% on the QALB 2015 native test data, and 83.6% on non-native data. This highlights the effectiveness of the proposed methodology in rectifying various grammatical errors in Arabic text. In conclusion, this study contributes to advancing the field of Arabic GEC by leveraging transfer learning with pre-trained transformers. The findings underscore the potential of this approach to overcome challenges posed by limited data availability, with AraBART emerging as a practical choice. This research opens avenues for further exploration in low-resource languages. It suggests potential applications in high-resource languages, encouraging future comparative studies."}, "subjects": ["Computer Science", "Artificial Intelligence", "Data Mining and Knowledge Discovery", "Probability and Statistics in Computer Science", "Computational Science and Engineering", "Image Processing and Computer Vision", "Computational Biology/Bioinformatics"], "disciplines": [{"id": "2970", "term": "Artificial Intelligence"}, {"id": "3820", "term": "Data Mining and Knowledge Discovery"}, {"id": "5673", "term": "Probability and Statistics in Computer Science"}, {"id": "4149", "term": "Computational Science and Engineering"}, {"id": "4777", "term": "Computer Vision"}, {"id": "2912", "term": "Computational and Systems Biology"}]}, {"contentType": "Article", "identifier": "doi:10.1038/s41598-025-99515-6", "language": "en", "url": [{"format": "", "platform": "", "value": "http://dx.doi.org/10.1038/s41598-025-99515-6"}], "title": "Emotion-Aware RoBERTa enhanced with emotion-specific attention and TF-IDF gating for fine-grained emotion recognition", "creators": [{"creator": "Alqarni, Fatimah"}, {"creator": "Sagheer, Alaa"}, {"creator": "Alabbad, Amira"}, {"creator": "Hamdoun, Hala"}], "publicationName": "Scientific Reports", "doi": "10.1038/s41598-025-99515-6", "publisher": "Nature", "publisherName": "Nature Publishing Group UK", "publicationDate": "2025-05-21", "publicationType": "Journal", "issn": "", "eIssn": "2045-2322", "volume": "15", "number": "1", "issueType": "Regular", "topicalCollection": "", "genre": ["OriginalPaper", "Article"], "startingPage": "1", "endingPage": "19", "journalId": "41598", "openAccess": "true", "onlineDate": "2025-05-21", "coverDate": "2025-12", "copyright": "\u00a92025 The Author(s)", "abstract": {"h1": "Abstract", "p": "Emotion recognition in text is a fundamental task in natural language processing, underpinning applications such as sentiment analysis, mental health monitoring, and content moderation. Although transformer-based models like RoBERTa have advanced contextual understanding in text, they still face limitations in identifying subtle emotional cues, handling class imbalances, and processing noisy or informal input. To address these challenges, this paper introduces Emotion-Aware RoBERTa, an enhanced framework that integrates an Emotion-Specific Attention (ESA) layer and a TF-IDF based gating mechanism. These additions are designed to dynamically prioritize emotionally salient tokens while suppressing irrelevant content, thereby improving both classification accuracy and robustness. The model achieved 96.77% accuracy and a weighted F1-score of 0.97 on the primary dataset, outperforming baseline RoBERTa and other benchmark models such as DistilBERT and ALBERT with a relative improvement ranging from 9.68% to 10.87%. Its generalization capability was confirmed across two external datasets, achieving 88.03% on a large-scale corpus and 65.67% on a smaller, noisier dataset. An ablation study revealed the complementary impact of the ESA and TF-IDF components, balancing performance and inference efficiency. Attention heatmaps were used to visualize ESA\u2019s ability to focus on key emotional expressions, while inference-time optimizations using FP16 and Automatic Mixed Precision (AMP) reduced memory consumption and latency. Additionally, McNemar\u2019s statistical test confirmed the significance of the improvements over the baseline. These findings demonstrate that Emotion-Aware RoBERTa offers a scalable, interpretable, and deployment-friendly solution for fine-grained emotion recognition, making it well-suited for real-world NLP applications in emotion-aware systems."}, "subjects": ["Science, Humanities and Social Sciences, multidisciplinary", "Science, Humanities and Social Sciences, multidisciplinary", "Science, multidisciplinary"], "disciplines": [{"id": "2887", "term": "Technology and Engineering"}, {"id": "2867", "term": "Physical Sciences"}, {"id": "2874", "term": "Life Sciences"}, {"id": "3001", "term": "Behavioral Sciences and Psychology"}, {"id": "2971", "term": "Computer Science"}]}, {"contentType": "Article", "identifier": "doi:10.1007/s10462-025-11237-3", "language": "en", "url": [{"format": "", "platform": "", "value": "http://dx.doi.org/10.1007/s10462-025-11237-3"}], "title": "Transformer-enhanced hierarchical encoding with multi-decoder for diversified MCQ distractor generation", "creators": [{"ORCID": "0000-0002-9228-8552", "creator": "Dong, Xiaohui"}, {"creator": "Li, Zhengluo"}, {"creator": "Su, Haoming"}, {"creator": "Xue, Jixiang"}, {"creator": "Dang, Xiaochao"}], "publicationName": "Artificial Intelligence Review", "doi": "10.1007/s10462-025-11237-3", "publisher": "Springer", "publisherName": "Springer Netherlands", "publicationDate": "2025-05-03", "publicationType": "Journal", "issn": "", "eIssn": "1573-7462", "volume": "58", "number": "8", "issueType": "Regular", "topicalCollection": "", "genre": "OriginalPaper", "startingPage": "1", "endingPage": "24", "journalId": "10462", "openAccess": "true", "onlineDate": "2025-05-03", "coverDate": "2025-08", "copyright": "\u00a92025 The Author(s)", "abstract": {"h1": "Abstract", "p": "The validity of multiple-choice questions (MCQs) in reading comprehension assessments relies heavily on the quality of the distractors. However, the manual design of these distractors is both time-consuming and costly, prompting researchers to turn to computer technology for the automatic generation of distractors. This task involves the process of taking a reading comprehension article, a question and its corresponding correct answer as input, with the goal of generating distractors that are related to the answer, semantically consistent with the question, and traceable within the article. Initially, heuristic rule-based approaches were employed, to generate only word-level or phrase-level distractors. Recent studies have shifted towards using sequence-to-sequence neural networks for sentence-level distractor generation. Despite these advancements, these methods face two key challenges: difficulty in capturing long-distance semantic relationships within the context, leading to overly general or context-independent distractors, and the tendency for the generated distractors to be semantically similar. To address these limitations, this paper proposes a Transformer-Enhanced Hierarchical Encoding with Multi-Decoder (THE-MD) network, composed of a hierarchical encoder and multiple decoders. Specifically, the encoder employs the Transformer architecture to encode the context and capture long-range semantic information, thereby generating more contextually relevant distractors. The decoder utilizes multiple decoding strategies and a dissimilarity loss function to collaboratively generate diverse distractors. The experimental results show that the THE-MD model outperforms existing baselines on both automatic and manual evaluation metrics. On the RACE and RACE++ datasets, the model increased the BLEU-4 scores to 7.45 and 10.60, and the ROUGE-L scores to 22.96 and 34.88, while also demonstrating excellent performance in fluency and coherence metrics. These improvements highlight their potential to enhance the generation of MCQ distractors in educational assessments."}, "subjects": ["Computer Science", "Artificial Intelligence", "Computer Science, general"], "disciplines": [{"id": "2970", "term": "Artificial Intelligence"}, {"id": "2971", "term": "Computer Science"}]}, {"contentType": "Article", "identifier": "doi:10.1038/s41598-025-97500-7", "language": "en", "url": [{"format": "", "platform": "", "value": "http://dx.doi.org/10.1038/s41598-025-97500-7"}], "title": "Cross language transformation of free text into structured lobectomy surgical records from a multi center study", "creators": [{"creator": "Yang, Xiongwen"}, {"creator": "Xiao, Yi"}, {"creator": "Liu, Di"}, {"creator": "Deng, Huiyin"}, {"creator": "Huang, Jian"}, {"creator": "Zhou, Yubin"}, {"creator": "Dai, Chuanzhou"}, {"creator": "Wu, Jun"}, {"creator": "Liu, Dan"}, {"creator": "Liang, Maoli"}, {"creator": "Xu, Chuan"}], "publicationName": "Scientific Reports", "doi": "10.1038/s41598-025-97500-7", "publisher": "Nature", "publisherName": "Nature Publishing Group UK", "publicationDate": "2025-05-02", "publicationType": "Journal", "issn": "", "eIssn": "2045-2322", "volume": "15", "number": "1", "issueType": "Regular", "topicalCollection": "", "genre": ["OriginalPaper", "Article"], "startingPage": "1", "endingPage": "11", "journalId": "41598", "openAccess": "true", "onlineDate": "2025-05-02", "coverDate": "2025-12", "copyright": "\u00a92025 The Author(s)", "abstract": {"h1": "Abstract", "p": "In a recent study, the effectiveness of GPT-4 Omni in transforming lobectomy surgical records into structured data across multiple languages was explored. The aim was to improve both efficiency and accuracy in documenting thoracic surgical oncology procedures. Involving 466 records from seven specialized hospitals, the process started with OCR and text normalization. A manual restructuring by thoracic oncologists set the benchmark for fine-tuning Generative Pre-trained Transformer 4 Omni (GPT-4o). Experts reviewed the AI\u2019s output, assessing it on accuracy, precision, recall, and F1 scores. GPT-4o demonstrated high performance across both Chinese and English records, achieving an accuracy of 0.966, precision of 0.981, recall of 0.982, and an F1-score of 0.982 in both language settings. Results showed that GPT-4o was highly effective in both Chinese and English, significantly speeding up documentation compared to traditional methods. While it performed well across languages and reduced review times, common error types included terminology misinterpretations (2.82%), procedural sequence errors (1.41%), and omissions of key details (0.47%). While it performed well across languages and reduced review times, these limitations highlight areas for further refinement, particularly in enhancing contextual understanding and mitigating minor errors. Nonetheless, GPT-4o shows great potential in standardizing surgical records, streamlining workflows, and boosting care and research in thoracic oncology."}, "subjects": ["Science, Humanities and Social Sciences, multidisciplinary", "Science, Humanities and Social Sciences, multidisciplinary", "Science, multidisciplinary"], "disciplines": [{"id": "2887", "term": "Technology and Engineering"}, {"id": "2867", "term": "Physical Sciences"}, {"id": "2874", "term": "Life Sciences"}, {"id": "3001", "term": "Behavioral Sciences and Psychology"}, {"id": "2971", "term": "Computer Science"}]}, {"contentType": "Article", "identifier": "doi:10.1038/s41598-025-87862-3", "language": "en", "url": [{"format": "", "platform": "", "value": "http://dx.doi.org/10.1038/s41598-025-87862-3"}], "title": "An LLM-based hybrid approach for enhanced automated essay scoring", "creators": [{"creator": "Atkinson, John"}, {"creator": "Palma, Diego"}], "publicationName": "Scientific Reports", "doi": "10.1038/s41598-025-87862-3", "publisher": "Nature", "publisherName": "Nature Publishing Group UK", "publicationDate": "2025-04-25", "publicationType": "Journal", "issn": "", "eIssn": "2045-2322", "volume": "15", "number": "1", "issueType": "Regular", "topicalCollection": "", "genre": ["OriginalPaper", "Article"], "startingPage": "1", "endingPage": "9", "journalId": "41598", "openAccess": "true", "onlineDate": "2025-04-25", "coverDate": "2025-12", "copyright": "\u00a92025 The Author(s)", "abstract": {"h1": "Abstract", "p": "Automated Essay Scoring systems have traditionally relied on shallow lexical data, such as word frequency and sentence length, to assess essays. However, these approaches neglect crucial factors like text structure and semantics, resulting in limited evaluations of coherence and quality. To address these limitations, we propose a hybrid approach to AES that combines multiple features from different linguistic levels. By leveraging the complementary nature of these features, our model captures the intricate relationships underlying coherent texts. Through extensive experimentation using standard essay datasets, we demonstrate that our large language model based hybrid model surpasses state-of-the-art methods based on shallow features and pure neural networks. This research represents a significant advancement towards the development of an accurate and effective tool for assessing student writing."}, "subjects": ["Science, Humanities and Social Sciences, multidisciplinary", "Science, Humanities and Social Sciences, multidisciplinary", "Science, multidisciplinary"], "disciplines": [{"id": "2887", "term": "Technology and Engineering"}, {"id": "2867", "term": "Physical Sciences"}, {"id": "2874", "term": "Life Sciences"}, {"id": "3001", "term": "Behavioral Sciences and Psychology"}, {"id": "2971", "term": "Computer Science"}]}, {"contentType": "Article", "identifier": "doi:10.1038/s41598-025-98483-1", "language": "en", "url": [{"format": "", "platform": "", "value": "http://dx.doi.org/10.1038/s41598-025-98483-1"}], "title": "Industrial applications of large language models", "creators": [{"creator": "Raza, Mubashar"}, {"creator": "Jahangir, Zarmina"}, {"creator": "Riaz, Muhammad Bilal"}, {"creator": "Saeed, Muhammad Jasim"}, {"creator": "Sattar, Muhammad Awais"}], "publicationName": "Scientific Reports", "doi": "10.1038/s41598-025-98483-1", "publisher": "Nature", "publisherName": "Nature Publishing Group UK", "publicationDate": "2025-04-21", "publicationType": "Journal", "issn": "", "eIssn": "2045-2322", "volume": "15", "number": "1", "issueType": "Regular", "topicalCollection": "", "genre": ["OriginalPaper", "Article"], "startingPage": "1", "endingPage": "23", "journalId": "41598", "openAccess": "true", "onlineDate": "2025-04-21", "coverDate": "2025-12", "copyright": "\u00a92025 The Author(s)", "abstract": {"h1": "Abstract", "p": "Large language models (LLMs) are artificial intelligence (AI) based computational models designed to understand and generate human like text. With billions of training parameters, LLMs excel in identifying intricate language patterns, enabling remarkable performance across a variety of natural language processing (NLP) tasks. After the introduction of transformer architectures, they are impacting the industry with their text generation capabilities. LLMs play an innovative role across various industries by automating NLP tasks. In healthcare, they assist in diagnosing diseases, personalizing treatment plans, and managing patient data. LLMs provide predictive maintenance in automotive industry. LLMs provide recommendation systems, and consumer behavior analyzers. LLMs facilitates researchers and offer personalized learning experiences in education. In finance and banking, LLMs are used for fraud detection, customer service automation, and risk management. LLMs are driving significant advancements across the industries by automating tasks, improving accuracy, and providing deeper insights. Despite these advancements, LLMs face challenges such as ethical concerns, biases in training data, and significant computational resource requirements, which must be addressed to ensure impartial and sustainable deployment. This study provides a comprehensive analysis of LLMs, their evolution, and their diverse applications across industries, offering researchers valuable insights into their transformative potential and the accompanying limitations."}, "subjects": ["Science, Humanities and Social Sciences, multidisciplinary", "Science, Humanities and Social Sciences, multidisciplinary", "Science, multidisciplinary"], "disciplines": [{"id": "2887", "term": "Technology and Engineering"}, {"id": "2867", "term": "Physical Sciences"}, {"id": "2874", "term": "Life Sciences"}, {"id": "3001", "term": "Behavioral Sciences and Psychology"}, {"id": "2971", "term": "Computer Science"}]}, {"contentType": "Article", "identifier": "doi:10.1186/s12911-025-02994-w", "language": "en", "url": [{"format": "", "platform": "", "value": "http://dx.doi.org/10.1186/s12911-025-02994-w"}], "title": "MISTIC: a novel approach for metastasis classification in Italian electronic health records using transformers", "creators": [{"creator": "Lilli, Livia"}, {"creator": "Santoro, Mario"}, {"creator": "Masiello, Valeria"}, {"creator": "Patarnello, Stefano"}, {"creator": "Tagliaferri, Luca"}, {"creator": "Marazzi, Fabio"}, {"creator": "Capocchiano, Nikola Dino"}], "publicationName": "BMC Medical Informatics and Decision Making", "doi": "10.1186/s12911-025-02994-w", "publisher": "BioMed Central", "publisherName": "BioMed Central", "publicationDate": "2025-04-10", "publicationType": "Journal", "issn": "", "eIssn": "1472-6947", "volume": "25", "number": "1", "issueType": "Regular", "topicalCollection": "Natural language processing in medical informatics", "genre": ["OriginalPaper", "Research"], "startingPage": "1", "endingPage": "11", "journalId": "12911", "openAccess": "true", "onlineDate": "2025-04-10", "coverDate": "2025-12", "copyright": "\u00a92025 The Author(s)", "abstract": {"h1": "Abstract", "p": ["Background", "Analysis of Electronic Health Records (EHRs) is crucial in real-world evidence (RWE), especially in oncology, as it provides valuable insights into the complex nature of the disease. The implementation of advanced techniques for automated extraction of structured information from textual data potentially enables access to expert knowledge in highly specialized contexts. In this paper, we introduce MISTIC, a Natural Language Processing (NLP) approach to classify the presence or absence of metastasis in Italian EHRs, in the breast cancer domain.", "Methods", "Our approach consists of a transformer-based framework designed for few-shot learning, requiring a small labelled dataset and minimal computational resources for training. The pipeline includes text segmentation to improve model processing and topic analysis to filter informative content, ensuring relevant input data for classification.", "Results", "MISTIC was evaluated across multiple data sources, and compared to several benchmark methodologies, ranging from a pattern-matching system, composed of regex and semantic rules, to BERT-based models implemented in a zero-shot learning setup and Large Language Models (LLMs). The results demonstrate the generalization of our approach, achieving an F-Score above 87% on all the sources, and outperforming the other experiments, with an overall F-Score of 91.2%.", "Conclusions", "MISTIC achieves high performance in the Italian metastasis classification task, outperforming rule-based systems, zero-shot BERT models, and LLMs. Its few-shot learning setup offers a computationally efficient alternative to large-scale models, while its segmentation and topic analysis steps enhance explainability by explicitly linking predictions to key textual elements. Furthermore, MISTIC demonstrates strong generalization across different data sources, reinforcing its potential as a scalable and transparent solution for clinical text classification. By extracting high-quality metastatic information from diverse textual data, MISTIC supports medical researchers in analyzing unstructured and highly informative content across a wide range of medical reports. In doing so, it enhances data accessibility and interpretability, addressing a critical gap in health informatics and clinical practice."]}, "subjects": ["Medicine & Public Health", "Health Informatics", "Information Systems and Communication Service", "Management of Computing and Information Systems"], "disciplines": [{"id": "4129", "term": "Health Informatics"}, {"id": "4767", "term": "Computer Engineering and Networks"}, {"id": "5793", "term": "IT Operations"}]}, {"contentType": "Article", "identifier": "doi:10.1007/s11548-024-03316-7", "language": "en", "url": [{"format": "", "platform": "", "value": "http://dx.doi.org/10.1007/s11548-024-03316-7"}], "title": "Comparison of active learning algorithms in classifying head computed tomography reports using bidirectional encoder representations from transformers", "creators": [{"ORCID": "0000-0002-1224-6601", "creator": "Wataya, Tomohiro"}, {"creator": "Miura, Azusa"}, {"creator": "Sakisuka, Takahisa"}, {"ORCID": "0000-0003-3423-7658", "creator": "Fujiwara, Masahiro"}, {"ORCID": "0000-0002-6148-0672", "creator": "Tanaka, Hisashi"}, {"creator": "Hiraoka, Yu"}, {"ORCID": "0000-0002-1206-2036", "creator": "Sato, Junya"}, {"creator": "Tomiyama, Miyuki"}, {"ORCID": "0000-0002-7006-3408", "creator": "Nishigaki, Daiki"}, {"ORCID": "0000-0002-5825-3348", "creator": "Kita, Kosuke"}, {"ORCID": "0000-0002-1572-5771", "creator": "Suzuki, Yuki"}, {"ORCID": "0000-0001-5293-4110", "creator": "Kido, Shoji"}, {"ORCID": "0000-0001-9918-7327", "creator": "Tomiyama, Noriyuki"}], "publicationName": "International Journal of Computer Assisted Radiology and Surgery", "doi": "10.1007/s11548-024-03316-7", "publisher": "Springer", "publisherName": "Springer International Publishing", "publicationDate": "2025-04-01", "publicationType": "Journal", "issn": "", "eIssn": "1861-6429", "volume": "20", "number": "4", "issueType": "Regular", "topicalCollection": "", "genre": ["OriginalPaper", "Original Article"], "startingPage": "687", "endingPage": "701", "journalId": "11548", "openAccess": "true", "printDate": "2025-04-27", "onlineDate": "2025-01-08", "coverDate": "2025-04", "copyright": "\u00a92024 The Author(s)", "abstract": {"h1": "Abstract", "p": ["Purpose", "Systems equipped with natural language (NLP) processing can reduce missed radiological findings by physicians, but the annotation costs are burden in the development. This study aimed to compare the effects of active learning (AL) algorithms in NLP for estimating the significance of head computed tomography (CT) reports using bidirectional encoder representations from transformers (BERT).", "Methods", "A total of 3728 head CT reports annotated with five categories of importance were used and UTH-BERT was adopted as the pre-trained BERT model. We assumed that 64% (2385 reports) of the data were initially in the unlabeled data pool (UDP), while the labeled data set (LD) used to train the model was empty. Twenty-five reports were repeatedly selected from the UDP and added to the LD, based on seven metrices: random sampling (RS: control), four uncertainty sampling (US) methods (least confidence (LC), margin sampling (MS), ratio of confidence (RC), and entropy sampling (ES)), and two distance-based sampling (DS) methods (cosine distance (CD) and Euclidian distance (ED)). The transition of accuracy of the model was evaluated using the test dataset.", "Results", "The accuracy of the models with US was significantly higher than RS when reports in LD were\u2009<\u20091800, whereas DS methods were significantly lower than RS. Among the US methods, MS and RC were even better than the others. With the US methods, the required labeled data decreased by 15.4\u201340.5%, and most efficient in RC. In addition, in the US methods, data for minor categories tended to be added to LD earlier than RS and DS.", "Conclusions", "In the classification task for the importance of head CT reports, US methods, especially RC and MS can lead to the effective fine-tuning of BERT models and reduce the imbalance of categories. AL can contribute to other studies on larger datasets by providing effective annotation."]}, "subjects": ["Medicine & Public Health", "Imaging / Radiology", "Surgery", "Health Informatics", "Computer Imaging, Vision, Pattern Recognition and Graphics", "Computer Science, general"], "disciplines": [{"id": "3270", "term": "Biological Imaging"}, {"id": "3070", "term": "Surgery"}, {"id": "4129", "term": "Health Informatics"}, {"id": "4631", "term": "Computer Imaging, Vision, Pattern Recognition and Graphics"}, {"id": "2971", "term": "Computer Science"}]}, {"contentType": "Article", "identifier": "doi:10.1007/s00330-025-11500-9", "language": "en", "url": [{"format": "", "platform": "", "value": "http://dx.doi.org/10.1007/s00330-025-11500-9"}], "title": "Impact of hospital-specific domain adaptation on BERT-based models to classify neuroradiology reports", "creators": [{"creator": "Agarwal, Siddharth"}, {"creator": "Wood, David"}, {"creator": "Murray, Benjamin A. K."}, {"creator": "Wei, Yiran"}, {"creator": "Busaidi, Ayisha Al"}, {"creator": "Kafiabadi, Sina"}, {"creator": "Guilhem, Emily"}, {"creator": "Lynch, Jeremy"}, {"creator": "Townend, Matthew"}, {"creator": "Mazumder, Asif"}, {"creator": "Barker, Gareth J."}, {"creator": "Cole, James H."}, {"creator": "Sasieni, Peter"}, {"creator": "Ourselin, Sebastien"}, {"creator": "Modat, Marc"}, {"ORCID": "0000-0003-0984-3998", "creator": "Booth, Thomas C."}], "publicationName": "European Radiology", "doi": "10.1007/s00330-025-11500-9", "publisher": "Springer", "publisherName": "Springer Berlin Heidelberg", "publicationDate": "2025-03-17", "publicationType": "Journal", "issn": "", "eIssn": "1432-1084", "volume": "", "number": "", "issueType": "", "topicalCollection": "", "genre": ["OriginalPaper", "Imaging Informatics and Artificial Intelligence"], "startingPage": "1", "endingPage": "15", "journalId": "330", "openAccess": "true", "onlineDate": "2025-03-17", "copyright": "\u00a92025 The Author(s)", "abstract": {"h1": "Abstract", "p": ["Objectives", "To determine the effectiveness of hospital-specific domain adaptation through masked language modelling (MLM) on BERT-based models\u2019 performance in classifying neuroradiology reports, and to compare these models with open-source large language models (LLMs).", "Materials and methods", "This retrospective study (2008\u20132019) utilised 126,556 and 86,032 MRI brain reports from two tertiary hospitals\u2014King\u2019s College Hospital (KCH) and Guys and St Thomas\u2019 Trust (GSTT). Various BERT-based models, including RoBERTa, BioBERT and RadBERT, underwent MLM on unlabelled reports from these centres. The downstream tasks were binary abnormality classification and multi-label classification. Performances of models with and without hospital-specific domain adaptation were compared against each other and LLMs on internal (KCH) and external (GSTT) hold-out test sets. Model performances for binary classification were compared using 2-way and 1-way ANOVA.", "Results", "All models that underwent hospital-specific domain adaptation performed better than their baseline counterparts (all p -values\u2009<\u20090.001). For binary classification, MLM on all available unlabelled reports (194,467 reports) yielded the highest balanced accuracies (KCH: mean 97.0\u2009\u00b1\u20090.4% (standard deviation), GSTT: 95.5\u2009\u00b1\u20091.0%), after which no differences between BERT-based models remained (1-way ANOVA, p -values\u2009>\u20090.05). There was a log-linear relationship between the number of reports and performance. LLama-3.0 70B was the best-performing LLM (KCH: 97.1%, GSTT: 94.0%). Multi-label classification demonstrated consistent performance improvements from MLM for all abnormality categories.", "Conclusion", "Hospital-specific domain adaptation should be considered best practice when deploying BERT-based models in new clinical settings. When labelled data is scarce or unavailable, LLMs can serve as a viable alternative, assuming adequate computational power is accessible.", "Key Points", "Question BERT-based models can classify radiology reports, but it is unclear if there is any incremental benefit from additional hospital-specific domain adaptation . Findings Hospital-specific domain adaptation resulted in the highest BERT-based model accuracies and performance scaled log-linearly with the number of reports . Clinical relevance BERT-based models after hospital-specific domain adaptation achieve the best classification results provided sufficient high-quality training labels. When labelled data is scarce, LLMs such as Llama-3.0 70B are a viable alternative provided there are sufficient computational resources .", "Graphical Abstract", ""]}, "subjects": ["Medicine & Public Health", "Imaging / Radiology", "Diagnostic Radiology", "Interventional Radiology", "Neuroradiology", "Ultrasound", "Internal Medicine"], "disciplines": [{"id": "3270", "term": "Biological Imaging"}, {"id": "2963", "term": "Radiology"}, {"id": "5301", "term": "Interventional Radiology"}, {"id": "7122", "term": "Neuroradiology"}, {"id": "8169", "term": "Ultrasonics"}, {"id": "2992", "term": "Internal Medicine"}]}, {"contentType": "Article", "identifier": "doi:10.1007/s10462-025-11162-5", "language": "en", "url": [{"format": "", "platform": "", "value": "http://dx.doi.org/10.1007/s10462-025-11162-5"}], "title": "BERT applications in natural language processing: a review", "creators": [{"creator": "Gardazi, Nadia Mushtaq"}, {"creator": "Daud, Ali"}, {"creator": "Malik, Muhammad Kamran"}, {"creator": "Bukhari, Amal"}, {"creator": "Alsahfi, Tariq"}, {"creator": "Alshemaimri, Bader"}], "publicationName": "Artificial Intelligence Review", "doi": "10.1007/s10462-025-11162-5", "publisher": "Springer", "publisherName": "Springer Netherlands", "publicationDate": "2025-03-15", "publicationType": "Journal", "issn": "", "eIssn": "1573-7462", "volume": "58", "number": "6", "issueType": "Regular", "topicalCollection": "", "genre": "OriginalPaper", "startingPage": "1", "endingPage": "49", "journalId": "10462", "openAccess": "true", "onlineDate": "2025-03-15", "coverDate": "2025-06", "copyright": "\u00a92025 The Author(s)", "abstract": {"h1": "Abstract", "p": "BERT (Bidirectional Encoder Representations from Transformers) has revolutionized Natural Language Processing (NLP) by significantly enhancing the capabilities of language models. This review study examines the complex nature of BERT, including its structure, utilization in different NLP tasks, and the further development of its design via modifications. The study thoroughly analyses the methodological aspects, conducting a comprehensive analysis of the planning process, the implemented procedures, and the criteria used to decide which data to include or exclude in the evaluation framework. In addition, the study thoroughly examines the influence of BERT on several NLP tasks, such as Sentence Boundary Detection, Tokenization, Grammatical Error Detection and Correction, Dependency Parsing, Named Entity Recognition, Part of Speech Tagging, Question Answering Systems, Machine Translation, Sentiment analysis, fake review detection and Cross-lingual transfer learning. The review study adds to the current literature by integrating ideas from multiple sources, explicitly emphasizing the problems and prospects in BERT-based models. The objective is to comprehensively comprehend BERT and its implementations, targeting both experienced researchers and novices in the domain of NLP. Consequently, the present study is expected to inspire more research endeavors, promote innovative adaptations of BERT, and deepen comprehension of its extensive capabilities in various NLP applications. The results presented in this research are anticipated to influence the advancement of future language models and add to the ongoing discourse on enhancing technology for understanding natural language."}, "subjects": ["Computer Science", "Artificial Intelligence", "Computer Science, general"], "disciplines": [{"id": "2970", "term": "Artificial Intelligence"}, {"id": "2971", "term": "Computer Science"}]}, {"contentType": "Article", "identifier": "doi:10.1007/s10506-023-09380-9", "language": "en", "url": [{"format": "", "platform": "", "value": "http://dx.doi.org/10.1007/s10506-023-09380-9"}], "title": "A neural network to identify requests, decisions, and arguments in court rulings on custody", "creators": [{"ORCID": "0000-0001-5181-1119", "creator": "Mu\u00f1oz-Soro, Jos\u00e9 F\u00e9lix"}, {"ORCID": "0000-0003-2755-5500", "creator": "Hoyo Alonso, Rafael"}, {"creator": "Monta\u00f1es, Rosa"}, {"creator": "Lacueva, Francisco"}], "publicationName": "Artificial Intelligence and Law", "doi": "10.1007/s10506-023-09380-9", "publisher": "Springer", "publisherName": "Springer Netherlands", "publicationDate": "2025-03-01", "publicationType": "Journal", "issn": "0924-8463", "eIssn": "1572-8382", "volume": "33", "number": "1", "issueType": "Regular", "topicalCollection": "", "genre": ["OriginalPaper", "Original Research"], "startingPage": "101", "endingPage": "135", "journalId": "10506", "openAccess": "true", "printDate": "2025-03-05", "onlineDate": "2024-01-09", "coverDate": "2025-03", "copyright": "\u00a92024 The Author(s)", "abstract": {"h1": "Abstract", "p": "Court rulings are among the most important documents in all legal systems. This article describes a study in which natural language processing is used for the automatic characterization of Spanish judgments that deal with the physical custody (joint or individual) of minors. The model was trained to identify a set of elements: the type of custody requested by the plaintiff, the type of custody decided on by the court, and eight of the most commonly used arguments in this type of judgment. Two jurists independently annotated more than 3000 judgments, which were used to train a model based on transformers. The main difficulties encountered in this task were the complexity of the judicial language and the need to work with appellate court rulings that have a more complicated structure than decisions at first instance. For the complete court rulings, the F1 score of the inter-annotator agreement ranged from 0.60 to 0.86 and the Kappa index from 0.33 to 0.73. The F1 score of the agreement between the model and the annotators ranged from 0.66 to 0.93 and the Kappa index from 0.57 to 0.80. These results in which the model performance exceeds even the inter-annotator agreement show the high ability of transformers to identify abstract entities in legal texts."}, "subjects": ["Computer Science", "Artificial Intelligence", "IT Law, Media Law, Intellectual Property", "Philosophy of Law", "Legal Aspects of Computing", "Information Storage and Retrieval"], "disciplines": [{"id": "2970", "term": "Artificial Intelligence"}, {"id": "4575", "term": "IT Law, Media Law, Intellectual Property"}, {"id": "6628", "term": "Philosophy of Law"}, {"id": "7333", "term": "Legal Aspects of Computing"}, {"id": "5864", "term": "Information Storage and Retrieval"}]}, {"contentType": "Article", "identifier": "doi:10.1007/s44326-024-00043-w", "language": "en", "url": [{"format": "", "platform": "", "value": "http://dx.doi.org/10.1007/s44326-024-00043-w"}], "title": "The journey from natural language processing to large language models: key insights for radiologists", "creators": [{"ORCID": "0000-0002-4003-3320", "creator": "Fanni, Salvatore Claudio"}, {"creator": "Tumminello, Lorenzo"}, {"creator": "Formica, Valentina"}, {"creator": "Caputo, Francesca Pia"}, {"creator": "Aghakhanyan, Gayane"}, {"creator": "Ambrosini, Ilaria"}, {"creator": "Francischello, Roberto"}, {"creator": "Faggioni, Lorenzo"}, {"creator": "Cioni, Dania"}, {"creator": "Neri, Emanuele"}], "publicationName": "Journal of Medical Imaging and Interventional Radiology", "doi": "10.1007/s44326-024-00043-w", "publisher": "Springer", "publisherName": "Springer International Publishing", "publicationDate": "2024-12-19", "publicationType": "Journal", "issn": "", "eIssn": "3004-8613", "volume": "11", "number": "1", "issueType": "Regular", "topicalCollection": "", "genre": ["ReviewPaper", "Review Article"], "startingPage": "1", "endingPage": "10", "journalId": "44326", "openAccess": "true", "onlineDate": "2024-12-19", "coverDate": "2024-12", "copyright": "\u00a92024 The Author(s)", "abstract": {"h1": "Abstract", "p": "Artificial intelligence (AI) has undergone cycles of enthusiasm and stagnation, often referred to as \u201cAI winters.\u201d The introduction of large language models (LLMs), such as OpenAI\u2019s ChatGPT in late 2022, has revitalized interest in AI, particularly within health-care applications, including radiology. The roots of AI in language processing can be traced back to Alan Turing\u2019s 1950 work, which established foundational principles for natural language processing (NLP). Early iterations of NLP primarily concentrated on natural language understanding (NLU) and natural language generation (NLG), but they faced significant challenges related to contextual comprehension and the handling of lengthy text sequences. Recent advancements in NLP have demonstrated considerable promise in automating the analysis of unstructured data, including electronic health records and radiology reports. LLMs, which are based on the transformer architecture introduced in 2017, excel at capturing complex language dependencies and facilitating tasks, such as report generation and clinical decision support. This review critically examines the evolution from traditional NLP to LLMs, highlighting their transformative potential within the field of radiology. Despite the advantages presented by LLMs, challenges persist, including concerns regarding data privacy, the potential for generating misinformation, and the imperative for rigorous validation protocols. Addressing these challenges is crucial for harnessing the full potential of LLMs to enhance diagnostic precision and workflow efficiency in radiology, ultimately improving patient care and outcomes."}, "subjects": ["Medicine & Public Health", "Imaging / Radiology", "Diagnostic Radiology", "Interventional Radiology", "Neuroradiology", "Ultrasound"], "disciplines": [{"id": "3270", "term": "Biological Imaging"}, {"id": "2963", "term": "Radiology"}, {"id": "5301", "term": "Interventional Radiology"}, {"id": "7122", "term": "Neuroradiology"}, {"id": "8169", "term": "Ultrasonics"}]}, {"contentType": "Article", "identifier": "doi:10.1186/s12911-024-02814-7", "language": "en", "url": [{"format": "", "platform": "", "value": "http://dx.doi.org/10.1186/s12911-024-02814-7"}], "title": "Uncertainty-aware automatic TNM staging classification for [^18F] Fluorodeoxyglucose PET-CT reports for lung cancer utilising transformer-based language models and multi-task learning", "creators": [{"creator": "Barlow, Stephen H."}, {"creator": "Chicklore, Sugama"}, {"creator": "He, Yulan"}, {"creator": "Ourselin, Sebastien"}, {"creator": "Wagner, Thomas"}, {"creator": "Barnes, Anna"}, {"creator": "Cook, Gary J.R."}], "publicationName": "BMC Medical Informatics and Decision Making", "doi": "10.1186/s12911-024-02814-7", "publisher": "BioMed Central", "publisherName": "BioMed Central", "publicationDate": "2024-12-18", "publicationType": "Journal", "issn": "", "eIssn": "1472-6947", "volume": "24", "number": "1", "issueType": "Regular", "topicalCollection": "Natural language processing in medical informatics", "genre": ["OriginalPaper", "Research"], "startingPage": "1", "endingPage": "14", "journalId": "12911", "openAccess": "true", "onlineDate": "2024-12-18", "coverDate": "2024-12", "copyright": "\u00a92024 The Author(s)", "abstract": {"h1": "Abstract", "p": ["Background", "[^18F] Fluorodeoxyglucose (FDG) PET-CT is a clinical imaging modality widely used in diagnosing and staging lung cancer. The clinical findings of PET-CT studies are contained within free text reports, which can currently only be categorised by experts manually reading them. Pre-trained transformer-based language models (PLMs) have shown success in extracting complex linguistic features from text. Accordingly, we developed a multi-task \u2018TNMu\u2019 classifier to classify the presence/absence of tumour, node, metastasis (\u2018TNM\u2019) findings (as defined by The Eight Edition of TNM Staging for Lung Cancer). This is combined with an uncertainty classification task (\u2018u\u2019) to account for studies with ambiguous TNM status.", "Methods", "2498 reports were annotated by a nuclear medicine physician and split into train, validation, and test datasets. For additional evaluation an external dataset ( n \u2009=\u2009461 reports) was created, and annotated by two nuclear medicine physicians with agreement reached on all examples. We trained and evaluated eleven publicly available PLMs to determine which is most effective for PET-CT reports, and compared multi-task, single task and traditional machine learning approaches.", "Results", "We find that a multi-task approach with GatorTron as PLM achieves the best performance, with an overall accuracy (all four tasks correct) of 84% and a Hamming loss of 0.05 on the internal test dataset, and 79% and 0.07 on the external test dataset. Performance on the individual TNM tasks approached expert performance with macro average F1 scores of 0.91, 0.95 and 0.90 respectively on external data. For uncertainty an F1 of 0.77 is achieved.", "Conclusions", "Our \u2018TNMu\u2019 classifier successfully extracts TNM staging information from internal and external PET-CT reports. We concluded that multi-task approaches result in the best performance, and better computational efficiency over single task PLM approaches. We believe these models can improve PET-CT services by assisting in auditing, creating research cohorts, and developing decision support systems. Our approach to handling uncertainty represents a novel first step but has room for further refinement."]}, "subjects": ["Medicine & Public Health", "Health Informatics", "Information Systems and Communication Service", "Management of Computing and Information Systems"], "disciplines": [{"id": "4129", "term": "Health Informatics"}, {"id": "4767", "term": "Computer Engineering and Networks"}, {"id": "5793", "term": "IT Operations"}]}, {"contentType": "Article", "identifier": "doi:10.1038/s41598-024-82192-2", "language": "en", "url": [{"format": "", "platform": "", "value": "http://dx.doi.org/10.1038/s41598-024-82192-2"}], "title": "GPT-4 shows potential for identifying social anxiety from clinical interview data", "creators": [{"ORCID": "0009-0005-3344-4753", "creator": "Ohse, Julia"}, {"ORCID": "0009-0003-1197-7255", "creator": "Had\u017ei\u0107, Bakir"}, {"ORCID": "0009-0001-7448-7857", "creator": "Mohammed, Parvez"}, {"ORCID": "0009-0008-9481-9354", "creator": "Peperkorn, Nicolina"}, {"ORCID": "0009-0000-9854-3780", "creator": "Fox, Janosch"}, {"ORCID": "0009-0008-1242-9070", "creator": "Krutzki, Joshua"}, {"ORCID": "0009-0005-9843-8776", "creator": "Lyko, Alexander"}, {"ORCID": "0000-0002-0492-4708", "creator": "Mingyu, Fan"}, {"ORCID": "0000-0001-9947-2742", "creator": "Zheng, Xiaohu"}, {"ORCID": "0000-0002-8254-8293", "creator": "R\u00e4tsch, Matthias"}, {"ORCID": "0000-0002-6281-0901", "creator": "Shiban, Youssef"}], "publicationName": "Scientific Reports", "doi": "10.1038/s41598-024-82192-2", "publisher": "Nature", "publisherName": "Nature Publishing Group UK", "publicationDate": "2024-12-16", "publicationType": "Journal", "issn": "", "eIssn": "2045-2322", "volume": "14", "number": "1", "issueType": "Regular", "topicalCollection": "", "genre": ["OriginalPaper", "Article"], "startingPage": "1", "endingPage": "12", "journalId": "41598", "openAccess": "true", "onlineDate": "2024-12-16", "coverDate": "2024-12", "copyright": "\u00a92024 The Author(s)", "abstract": {"h1": "Abstract", "p": "While the potential of Artificial Intelligence (AI)\u2014particularly Natural Language Processing (NLP) models\u2014for detecting symptoms of depression from text has been vastly researched, only a few studies examine such potential for the detection of social anxiety symptoms. We investigated the ability of the large language model (LLM) GPT-4 to correctly infer social anxiety symptom strength from transcripts obtained from semi-structured interviews. N \u2009=\u200951 adult participants were recruited from a convenience sample of the German population. Participants filled in a self-report questionnaire on social anxiety symptoms (SPIN) prior to being interviewed on a secure online teleconference platform. Transcripts from these interviews were then evaluated by GPT-4. GPT-4 predictions were highly correlated ( r \u2009=\u20090.79) with scores obtained on the social anxiety self-report measure. Following the cut-off conventions for this population, an F_1 accuracy score of 0.84 could be obtained. Future research should examine whether these findings hold true in larger and more diverse datasets."}, "subjects": ["Science, Humanities and Social Sciences, multidisciplinary", "Science, Humanities and Social Sciences, multidisciplinary", "Science, multidisciplinary"], "disciplines": [{"id": "2887", "term": "Technology and Engineering"}, {"id": "2867", "term": "Physical Sciences"}, {"id": "2874", "term": "Life Sciences"}, {"id": "3001", "term": "Behavioral Sciences and Psychology"}, {"id": "2971", "term": "Computer Science"}]}, {"contentType": "Article", "identifier": "doi:10.1007/s10772-024-10136-2", "language": "en", "url": [{"format": "", "platform": "", "value": "http://dx.doi.org/10.1007/s10772-024-10136-2"}], "title": "A transformer-based approach to Nigerian Pidgin text generation", "creators": [{"creator": "Garba, Kabir"}, {"ORCID": "0000-0001-6780-2495", "creator": "Kolajo, Taiwo"}, {"creator": "Agbogun, Joshua B."}], "publicationName": "International Journal of Speech Technology", "doi": "10.1007/s10772-024-10136-2", "publisher": "Springer", "publisherName": "Springer US", "publicationDate": "2024-12-01", "publicationType": "Journal", "issn": "1381-2416", "eIssn": "1572-8110", "volume": "27", "number": "4", "issueType": "Regular", "topicalCollection": "", "genre": "OriginalPaper", "startingPage": "1027", "endingPage": "1037", "journalId": "10772", "openAccess": "true", "printDate": "2024-12-16", "onlineDate": "2024-10-16", "coverDate": "2024-12", "copyright": "\u00a92024 The Author(s)", "abstract": {"h1": "Abstract", "p": "This paper describes the development of a transformer-based text generation model for Nigerian Pidgin also known as Naij\u00e1, a popular language in West Africa. Despite its wide use, Nigerian Pidgin remains under-resourced, particularly in areas related to text generation and natural language processing. These difficulties are primarily due to technological constraints rather than the language\u2019s fundamental attributes. There is currently a demand for Nigerian Pidgin-specific solutions because it is used in everyday communication and has a unique linguistic blend. This paper aims to close this gap by exploring the application of state-of-the-art transformer technology to develop a text generation model for Nigerian Pidgin. This work uses the public Afriberta-corpus dataset to optimize the Generative Pre-trained Transformer (GPT-2) model across a sizeable dataset. The performance evaluators, BLEU and Perplexity metrics provide a detailed breakdown of the model\u2019s text quality and predictive accuracy. Despite the difficulties caused by a limited amount of training data, preliminary evaluations show that the model can generate coherent Nigerian Pidgin text. The performance evaluation yielded perplexity scores of 43.56 for variable target reference length and 43.26 for fixed text length. BLEU scores of 0.15 for fixed max length and 0.56 for variable reference target length. This highlights the quality of generated text and the significant improvement when the generated text length is aligned with the reference target. Our work was benchmarked against African American Vernacular (AAVE) revealing that BLEU scores for AAVE are significantly lower than those for Standard American English, with BLEU given as 0.26. Our Nigerian Pidgin model, with a BLEU score of 0.56, shows a better performance. However, both results suggest that both dialects are challenging for language models. Leveraging the pre-trained transformer-based language model and evaluation metrics, we showcase the model\u2019s capacity for coherent Nigerian Pidgin text generation. For future research, the research work can serve as a good foundation for advancement and progress in the Nigerian Pidgin language generation and other low-resource languages."}, "subjects": ["Engineering", "Signal,Image and Speech Processing", "Social Sciences, general", "Artificial Intelligence"], "disciplines": [{"id": "2885", "term": "Signal, Speech and Image Processing"}, {"id": "2891", "term": "Society"}, {"id": "2970", "term": "Artificial Intelligence"}]}, {"contentType": "Article", "identifier": "doi:10.1007/s10462-024-10997-8", "language": "en", "url": [{"format": "", "platform": "", "value": "http://dx.doi.org/10.1007/s10462-024-10997-8"}], "title": "Uncovering suggestions in MOOC discussion forums: a transformer-based approach", "creators": [{"ORCID": "0000-0002-5316-3810", "creator": "Reina S\u00e1nchez, Karen"}, {"creator": "Vaca Serrano, Gonzalo"}, {"ORCID": "0000-0003-2597-3784", "creator": "Arb\u00e1izar G\u00f3mez, Juan Pedro"}, {"ORCID": "0000-0001-8018-6391", "creator": "Duran-Heras, Alfonso"}], "publicationName": "Artificial Intelligence Review", "doi": "10.1007/s10462-024-10997-8", "publisher": "Springer", "publisherName": "Springer Netherlands", "publicationDate": "2024-11-04", "publicationType": "Journal", "issn": "", "eIssn": "1573-7462", "volume": "58", "number": "1", "issueType": "Regular", "topicalCollection": "", "genre": "OriginalPaper", "startingPage": "1", "endingPage": "23", "journalId": "10462", "openAccess": "true", "onlineDate": "2024-11-04", "coverDate": "2025-01", "copyright": "\u00a92024 The Author(s)", "abstract": {"h1": "Abstract", "p": "The field of natural language processing has experienced significant advances in recent years, but these advances have not yet resulted in improved analytics for instructors on MOOC platforms. Valuable information, such as suggestions, is generated in the comment forums of these courses, but due to their volume, manual processing is often impractical. This study examines the feasibility of fine-tuning and effectively utilizing state-of-the-art deep learning models to identify comments that contain suggestions in MOOC forums. The main challenges encountered are the lack of labeled datasets from the MOOC context for fine-tuning classification models and the soaring computational cost of this training. For this study, we manually collected and labeled 2228 comments in Spanish and English from 5 MOOCs and scraped 1.4 million MOOC reviews from 3 platforms. We fine-tuned and evaluated 4 pretrained models based on the transformer architecture and 3 traditional machine learning models to compare their effectiveness in the suggestion mining task in this domain. Transformer-based models proved to be highly effective in this task/domain combination, achieving performance levels that matched or exceeded those deemed appropriate in other contexts and were significantly greater than those achieved by traditional models. Domain adaptation led to improved linguistic understanding of the target domain; however, in this project, this approach did not translate into an observable improvement in suggestion mining. The automated identification of comments that can be labeled as suggestions can result in considerable time savings for instructors, especially considering that less than a quarter of the analyzed comments contain suggestions."}, "subjects": ["Computer Science", "Artificial Intelligence", "Computer Science, general"], "disciplines": [{"id": "2970", "term": "Artificial Intelligence"}, {"id": "2971", "term": "Computer Science"}]}, {"contentType": "Article", "identifier": "doi:10.1007/s13139-024-00876-z", "language": "en", "url": [{"format": "", "platform": "", "value": "http://dx.doi.org/10.1007/s13139-024-00876-z"}], "title": "How to Harness the Power of GPT for Scientific Research: A Comprehensive Review of Methodologies, Applications, and Ethical Considerations", "creators": [{"ORCID": "0000-0003-3610-1957", "creator": "Park, Ki-Seong"}, {"creator": "Choi, Hongyoon"}], "publicationName": "Nuclear Medicine and Molecular Imaging", "doi": "10.1007/s13139-024-00876-z", "publisher": "Springer", "publisherName": "Springer Nature Singapore", "publicationDate": "2024-10-01", "publicationType": "Journal", "issn": "1869-3474", "eIssn": "1869-3482", "volume": "58", "number": "6", "issueType": "Regular", "topicalCollection": "", "genre": ["ReviewPaper", "Review"], "startingPage": "323", "endingPage": "331", "journalId": "13139", "openAccess": "true", "printDate": "2024-09-20", "onlineDate": "2024-08-12", "coverDate": "2024-10", "copyright": "\u00a92024 The Authors", "abstract": {"h1": "Abstract", "p": "The rapid advancements in natural language processing, particularly with the development of Generative Pre-trained Transformer (GPT) models, have opened up new avenues for researchers across various domains. This review article explores the potential of GPT as a research tool, focusing on the core functionalities, key features, and real-world applications of the GPT-4 model. We delve into the concept of prompt engineering, a crucial technique for effectively utilizing GPT, and provide guidelines for designing optimal prompts. Through case studies, we demonstrate how GPT can be applied at various stages of the research process, including literature review, data analysis, and manuscript preparation. The utilization of GPT is expected to enhance research efficiency, stimulate creative thinking, facilitate interdisciplinary collaboration, and increase the impact of research findings. However, it is essential to view GPT as a complementary tool rather than a substitute for human expertise, keeping in mind its limitations and ethical considerations. As GPT continues to evolve, researchers must develop a deep understanding of this technology and leverage its potential to advance their research endeavors while being mindful of its implications."}, "subjects": ["Medicine & Public Health", "Nuclear Medicine", "Imaging / Radiology", "Orthopedics", "Cardiology", "Oncology"], "disciplines": [{"id": "6262", "term": "Nuclear Medicine"}, {"id": "3270", "term": "Biological Imaging"}, {"id": "3927", "term": "Orthopaedics"}, {"id": "4347", "term": "Cardiology"}, {"id": "4047", "term": "Oncology"}]}, {"contentType": "Article", "identifier": "doi:10.1038/s41598-024-69664-1", "language": "en", "url": [{"format": "", "platform": "", "value": "http://dx.doi.org/10.1038/s41598-024-69664-1"}], "title": "Novel concept-based image captioning models using LSTM and multi-encoder transformer architecture", "creators": [{"creator": "Osman, Asmaa A. E."}, {"creator": "Shalaby, Mohamed A. Wahby"}, {"creator": "Soliman, Mona M."}, {"creator": "Elsayed, Khaled M."}], "publicationName": "Scientific Reports", "doi": "10.1038/s41598-024-69664-1", "publisher": "Nature", "publisherName": "Nature Publishing Group UK", "publicationDate": "2024-09-05", "publicationType": "Journal", "issn": "", "eIssn": "2045-2322", "volume": "14", "number": "1", "issueType": "Regular", "topicalCollection": "", "genre": ["OriginalPaper", "Article"], "startingPage": "1", "endingPage": "15", "journalId": "41598", "openAccess": "true", "onlineDate": "2024-09-05", "coverDate": "2024-12", "copyright": "\u00a92024 The Author(s)", "abstract": {"h1": "Abstract", "p": "Captioning an image involves using a combination of vision and language models to describe the image in an expressive and concise sentence. Successful captioning task requires extracting as much information as possible from the corresponding image. One of these key pieces of information is the topic to which the image belongs. The state-of-the-art methods used topic modeling depending only on caption text in order to extract these topics. The problem with extracting the topics using topic modeling only on caption text is that it lacks the consideration of the image\u2019s semantic information. Instead, concept modeling extracts the concepts directly from the images in addition to considering the corresponding caption text. Concept modeling can be used in image captioning to extremely capture the image contexts and benefit from it to produce more accurate descriptions. In this paper, novel image captioning models are proposed by utilizing the concept modeling technique. The first concept-based model is proposed by utilizing LSTM as a decoder while the second model is proposed in association with new multi-encoder transformer architecture. Standard metrics have been used to evaluate the proposed models using Microsoft COCO and Flickr30K datasets. The proposed models outperformed the related work methods with reduced computational complexity."}, "subjects": ["Science, Humanities and Social Sciences, multidisciplinary", "Science, Humanities and Social Sciences, multidisciplinary", "Science, multidisciplinary"], "disciplines": [{"id": "2887", "term": "Technology and Engineering"}, {"id": "2867", "term": "Physical Sciences"}, {"id": "2874", "term": "Life Sciences"}, {"id": "3001", "term": "Behavioral Sciences and Psychology"}, {"id": "2971", "term": "Computer Science"}]}, {"contentType": "Article", "identifier": "doi:10.1007/s12652-024-04824-9", "language": "en", "url": [{"format": "", "platform": "", "value": "http://dx.doi.org/10.1007/s12652-024-04824-9"}], "title": "A transformer-based Urdu image caption generation", "creators": [{"creator": "Hadi, Muhammad"}, {"creator": "Safder, Iqra"}, {"creator": "Waheed, Hajra"}, {"creator": "Zaman, Farooq"}, {"creator": "Aljohani, Naif Radi"}, {"creator": "Nawaz, Raheel"}, {"creator": "Hassan, Saeed Ul"}, {"ORCID": "0000-0002-0640-807X", "creator": "Sarwar, Raheem"}], "publicationName": "Journal of Ambient Intelligence and Humanized Computing", "doi": "10.1007/s12652-024-04824-9", "publisher": "Springer", "publisherName": "Springer Berlin Heidelberg", "publicationDate": "2024-09-01", "publicationType": "Journal", "issn": "1868-5137", "eIssn": "1868-5145", "volume": "15", "number": "9", "issueType": "Regular", "topicalCollection": "", "genre": ["OriginalPaper", "Original Research"], "startingPage": "3441", "endingPage": "3457", "journalId": "12652", "openAccess": "true", "printDate": "2024-08-19", "onlineDate": "2024-07-02", "coverDate": "2024-09", "copyright": "\u00a92024 The Author(s)", "abstract": {"h1": "Abstract", "p": "Image caption generation has emerged as a remarkable development that bridges the gap between Natural Language Processing (NLP) and Computer Vision (CV). It lies at the intersection of these fields and presents unique challenges, particularly when dealing with low-resource languages such as Urdu. Limited research on basic Urdu language understanding necessitates further exploration in this domain. In this study, we propose three Seq2Seq-based architectures specifically tailored for Urdu image caption generation. Our approach involves leveraging transformer models to generate captions in Urdu, a significantly more challenging task than English. To facilitate the training and evaluation of our models, we created an Urdu-translated subset of the flickr8k dataset, which contains images featuring dogs in action accompanied by corresponding Urdu captions. Our designed models encompassed a deep learning-based approach, utilizing three different architectures: Convolutional Neural Network (CNN) + Long Short-term Memory (LSTM) with Soft attention employing word2Vec embeddings, CNN+Transformer, and Vit+Roberta models. Experimental results demonstrate that our proposed model outperforms existing state-of-the-art approaches, achieving 86 BLEU-1 and 90 BERT-F1 scores. The generated Urdu image captions exhibit syntactic, contextual, and semantic correctness. Our study highlights the inherent challenges associated with retraining models on low-resource languages. Our findings highlight the potential of pre-trained models for facilitating the development of NLP and CV applications in low-resource language settings."}, "subjects": ["Engineering", "Computational Intelligence", "Artificial Intelligence", "Robotics and Automation", "User Interfaces and Human Computer Interaction"], "disciplines": [{"id": "5427", "term": "Computational Intelligence"}, {"id": "2970", "term": "Artificial Intelligence"}, {"id": "3185", "term": "Control, Robotics, Automation"}, {"id": "7147", "term": "User Interfaces and Human Computer Interaction"}]}, {"contentType": "Article", "identifier": "doi:10.1007/s13369-024-08845-6", "language": "en", "url": [{"format": "", "platform": "", "value": "http://dx.doi.org/10.1007/s13369-024-08845-6"}], "title": "Robust Drug Use Detection on X: Ensemble Method with a Transformer Approach", "creators": [{"ORCID": "0009-0006-1198-5986", "creator": "Al-Ghannam, Reem"}, {"creator": "Ykhlef, Mourad"}, {"creator": "Al-Dossari, Hmood"}], "publicationName": "Arabian Journal for Science and Engineering", "doi": "10.1007/s13369-024-08845-6", "publisher": "Springer", "publisherName": "Springer Berlin Heidelberg", "publicationDate": "2024-09-01", "publicationType": "Journal", "issn": "2193-567X", "eIssn": "2191-4281", "volume": "49", "number": "9", "issueType": "Regular", "topicalCollection": "", "genre": ["OriginalPaper", "Research Article-Computer Engineering and Computer Science "], "startingPage": "12867", "endingPage": "12885", "journalId": "13369", "openAccess": "true", "printDate": "2024-08-24", "onlineDate": "2024-03-14", "coverDate": "2024-09", "copyright": "\u00a92024 The Author(s)", "abstract": {"h1": "Abstract", "p": "There is a growing trend for groups associated with drug use to exploit social media platforms to propagate content that poses a risk to the population, especially those susceptible to drug use and addiction. Detecting drug-related social media content has become important for governments, technology companies, and those responsible for enforcing laws against proscribed drugs. Their efforts have led to the development of various techniques for identifying and efficiently removing drug-related content, as well as for blocking network access for those who create it. This study introduces a manually annotated Twitter dataset consisting of 112,057 tweets from 2008 to 2022, compiled for use in detecting associations connected with drug use. Working in groups, expert annotators classified tweets as either related or unrelated to drug use. The dataset was subjected to exploratory data analysis to identify its defining features. Several classification algorithms, including support vector machines, XGBoost, random forest, Naive Bayes, LSTM, and BERT, were used in experiments with this dataset. Among the baseline models, BERT with textual features achieved the highest F 1-score, at 0.9044. However, this performance was surpassed when the BERT base model and its textual features were concatenated with a deep neural network model, incorporating numerical and categorical features in the ensemble method, achieving an F 1-score of 0.9112. The Twitter dataset used in this study was made publicly available to promote further research and enhance the accuracy of the online classification of English-language drug-related content."}, "subjects": ["Engineering", "Engineering, general", "Science, Humanities and Social Sciences, multidisciplinary"], "disciplines": [{"id": "2887", "term": "Technology and Engineering"}, {"id": "2883", "term": "Humanities and Social Sciences"}]}, {"contentType": "Article", "identifier": "doi:10.1007/s10462-024-10895-z", "language": "en", "url": [{"format": "", "platform": "", "value": "http://dx.doi.org/10.1007/s10462-024-10895-z"}], "title": "From rule-based models to deep learning transformers architectures for natural language processing and sign language translation systems: survey, taxonomy and performance evaluation", "creators": [{"creator": "Shahin, Nada"}, {"creator": "Ismail, Leila"}], "publicationName": "Artificial Intelligence Review", "doi": "10.1007/s10462-024-10895-z", "publisher": "Springer", "publisherName": "Springer Netherlands", "publicationDate": "2024-08-29", "publicationType": "Journal", "issn": "", "eIssn": "1573-7462", "volume": "57", "number": "10", "issueType": "Regular", "topicalCollection": "", "genre": "OriginalPaper", "startingPage": "1", "endingPage": "51", "journalId": "10462", "openAccess": "true", "onlineDate": "2024-08-29", "coverDate": "2024-10", "copyright": "\u00a92024 The Author(s)", "abstract": {"h1": "Abstract", "p": "With the growing Deaf and Hard of Hearing population worldwide and the persistent shortage of certified sign language interpreters, there is a pressing need for an efficient, signs-driven, integrated end-to-end translation system, from sign to gloss to text and vice-versa. There has been a wealth of research on machine translations and related reviews. However, there are few works on sign language machine translation considering the particularity of the language being continuous and dynamic. This paper aims to address this void, providing a retrospective analysis of the temporal evolution of sign language machine translation algorithms and a taxonomy of the Transformers architectures, the most used approach in language translation. We also present the requirements of a real-time Quality-of-Service sign language machine translation system underpinned by accurate deep learning algorithms. We propose future research directions for sign language translation systems."}, "subjects": ["Computer Science", "Artificial Intelligence", "Computer Science, general"], "disciplines": [{"id": "2970", "term": "Artificial Intelligence"}, {"id": "2971", "term": "Computer Science"}]}, {"contentType": "Article", "identifier": "doi:10.1186/s12859-024-05903-6", "language": "en", "url": [{"format": "", "platform": "", "value": "http://dx.doi.org/10.1186/s12859-024-05903-6"}], "title": "VAIV bio-discovery service using transformer model and retrieval augmented generation", "creators": [{"creator": "Kim, Seonho"}, {"creator": "Yoon, Juntae"}], "publicationName": "BMC Bioinformatics", "doi": "10.1186/s12859-024-05903-6", "publisher": "BioMed Central", "publisherName": "BioMed Central", "publicationDate": "2024-08-21", "publicationType": "Journal", "issn": "", "eIssn": "1471-2105", "volume": "25", "number": "1", "issueType": "Regular", "topicalCollection": "Big data management in biological domains", "genre": ["OriginalPaper", "Database"], "startingPage": "1", "endingPage": "25", "journalId": "12859", "openAccess": "true", "onlineDate": "2024-08-21", "coverDate": "2024-12", "copyright": "\u00a92024 The Author(s)", "abstract": {"h1": "Abstract", "p": ["Background", "There has been a considerable advancement in AI technologies like LLM and machine learning to support biomedical knowledge discovery.", "Main body", "We propose a novel biomedical neural search service called \u2018VAIV Bio-Discovery\u2019, which supports enhanced knowledge discovery and document search on unstructured text such as PubMed. It mainly handles with information related to chemical compound/drugs, gene/proteins, diseases, and their interactions (chemical compounds/drugs-proteins/gene including drugs-targets, drug-drug, and drug-disease). To provide comprehensive knowledge, the system offers four search options: basic search, entity and interaction search, and natural language search. We employ T5slim_dec, which adapts the autoregressive generation task of the T5 (text-to-text transfer transformer) to the interaction extraction task by removing the self-attention layer in the decoder block. It also assists in interpreting research findings by summarizing the retrieved search results for a given natural language query with Retrieval Augmented Generation (RAG). The search engine is built with a hybrid method that combines neural search with the probabilistic search, BM25.", "Conclusion", "As a result, our system can better understand the context, semantics and relationships between terms within the document, enhancing search accuracy. This research contributes to the rapidly evolving biomedical field by introducing a new service to access and discover relevant knowledge."]}, "subjects": ["Life Sciences", "Bioinformatics", "Microarrays", "Computational Biology/Bioinformatics", "Computer Appl. in Life Sciences", "Algorithms"], "disciplines": [{"id": "7750", "term": "Bioinformatics"}, {"id": "2912", "term": "Computational and Systems Biology"}, {"id": "5024", "term": "Algorithms"}]}, {"contentType": "Article", "identifier": "doi:10.1007/s42979-024-03066-y", "language": "en", "url": [{"format": "", "platform": "", "value": "http://dx.doi.org/10.1007/s42979-024-03066-y"}], "title": "DiRecNetV2: A Transformer-Enhanced Network for Aerial Disaster Recognition", "creators": [{"ORCID": "0009-0005-8266-0727", "creator": "Shianios, Demetris"}, {"ORCID": "0000-0003-3981-993X", "creator": "Kolios, Panayiotis S."}, {"ORCID": "0000-0002-7926-7642", "creator": "Kyrkou, Christos"}], "publicationName": "SN Computer Science", "doi": "10.1007/s42979-024-03066-y", "publisher": "Springer", "publisherName": "Springer Nature Singapore", "publicationDate": "2024-08-08", "publicationType": "Journal", "issn": "", "eIssn": "2661-8907", "volume": "5", "number": "6", "issueType": "Regular", "topicalCollection": "Computer Analysis of Images and Patterns in the Deep Learning Era", "genre": ["OriginalPaper", "Original Research"], "startingPage": "1", "endingPage": "14", "journalId": "42979", "openAccess": "true", "onlineDate": "2024-08-08", "coverDate": "2024-08", "copyright": "\u00a92024 The Author(s)", "abstract": {"h1": "Abstract", "p": "The integration of Unmanned Aerial Vehicles (UAVs) with artificial intelligence (AI) models for aerial imagery processing in disaster assessment, necessitates models that demonstrate exceptional accuracy, computational efficiency, and real-time processing capabilities. Traditionally Convolutional Neural Networks (CNNs), demonstrate efficiency in local feature extraction but are limited by their potential for global context interpretation. On the other hand, Vision Transformers (ViTs) show promise for improved global context interpretation through the use of attention mechanisms, although they still remain underinvestigated in UAV-based disaster response applications. Bridging this research gap, we introduce DiRecNetV2, an improved hybrid model that utilizes convolutional and transformer layers. It merges the inductive biases of CNNs for robust feature extraction with the global context understanding of Transformers, maintaining a low computational load ideal for UAV applications. Additionally, we introduce a new, compact multi-label dataset of disasters, to set an initial benchmark for future research, exploring how models trained on single-label data perform in a multi-label test set. The study assesses lightweight CNNs and ViTs on the AIDERSv2 dataset, based on the frames per second (FPS) for efficiency and the weighted F1 scores for classification performance. DiRecNetV2 not only achieves a weighted F1 score of 0.964 on a single-label test set but also demonstrates adaptability, with a score of 0.614 on a complex multi-label test set, while functioning at 176.13 FPS on the Nvidia Orin Jetson device."}, "subjects": ["Computer Science", "Computer Science, general", "Computer Systems Organization and Communication Networks", "Software Engineering/Programming and Operating Systems", "Data Structures and Information Theory", "Information Systems and Communication Service", "Computer Imaging, Vision, Pattern Recognition and Graphics"], "disciplines": [{"id": "2971", "term": "Computer Science"}, {"id": "4767", "term": "Computer Engineering and Networks"}, {"id": "5482", "term": "Software Engineering"}, {"id": "2972", "term": "Data Structures and Information Theory"}, {"id": "4631", "term": "Computer Imaging, Vision, Pattern Recognition and Graphics"}]}]}
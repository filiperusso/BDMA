{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d35e8e0-8ed3-43df-99ff-d074ccbce734",
   "metadata": {},
   "source": [
    "**RANDOM FOREST (RF)**\n",
    "\n",
    "**DATA LOADING AND PREPARATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31d1dece-7ab4-460f-b147-96d4e170b293",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e621fda5-0943-4a16-bf4b-ef943cf6de6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1011 entries, 0 to 1010\n",
      "Columns: 828 entries, incident_id to 998\n",
      "dtypes: float64(826), int64(1), string(1)\n",
      "memory usage: 6.4 MB\n"
     ]
    }
   ],
   "source": [
    "# Read the CSV\n",
    "tfidf_df = pd.read_csv(\"tfidf_sncb.csv\", sep='\\,', engine='python')\n",
    "\n",
    "tfidf_df['incident_type'] = tfidf_df['incident_type'].astype('string') \n",
    "\n",
    "tfidf_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db029af2-2198-4aa1-b5a4-65741c9d6515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train_val_X pandas df has 808 rows and 826 columns.\n",
      "The test_y pandas series has 203 rows and 1 column.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Filter in the Features (the values acquired from the events sequence after TF-IDF)\n",
    "X = tfidf_df.drop(['incident_type', 'incident_id'], axis=1) \n",
    "\n",
    "# Filter in the Target variable (labels / incident types)\n",
    "y = tfidf_df['incident_type']  \n",
    "\n",
    "# setting random_state constant to be used in the whole pipeline and guarantee reproducibility\n",
    "r_state = 123\n",
    "\n",
    "# Split data into training+validation and testing sets\n",
    "train_val_X, test_X, train_val_y, test_y = train_test_split(X, \n",
    "                                                            y, \n",
    "                                                            train_size = 0.8, \n",
    "                                                            random_state = r_state, # setting random_state for reproducibility\n",
    "                                                            stratify = y) # to respect class imbalance in the label column\n",
    "\n",
    "print(f\"The train_val_X pandas df has {len(train_val_X)} rows and {len(train_val_X.columns)} columns.\")\n",
    "print(f\"The test_y pandas series has {len(test_y)} rows and 1 column.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6553064-17ab-4507-841d-28d033d59e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In RepeatedStratifiedKFold() function, the parameter n_splits has to be set atmost to 3, due to class imbalance in the label column.\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# get the size of the smallest incident type class\n",
    "value_counts = Counter(train_val_y)\n",
    "min_class_setsize = min(value_counts.values())\n",
    "\n",
    "print(f\"In RepeatedStratifiedKFold() function, the parameter n_splits has to be set atmost to {min_class_setsize}, due to class imbalance in the label column.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba71c940-735d-4b1f-b59c-ad73e542bbbb",
   "metadata": {},
   "source": [
    "**MODEL TRAINING AND VALIDATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fe905a6-33f9-4de8-9515-bd4d04b73cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "# Define the base model\n",
    "model_rf = RandomForestClassifier(random_state = r_state, \n",
    "                                  n_jobs = -1,\n",
    "                                  criterion = \"gini\", # tested [\"gini\", \"entropy\", \"log_loss\"]\n",
    "                                  max_features = 0.12, # tested [\"sqrt\", \"log2\", 10, 100, 0.12]\n",
    "                                  class_weight = None # tested [\"balanced\", \"balanced_subsample\"]\n",
    "                                 )\n",
    "\n",
    "# Set up cross-validation\n",
    "rskf = RepeatedStratifiedKFold(n_splits = min_class_setsize, \n",
    "                               n_repeats = 34, \n",
    "                               random_state = r_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc889ca3-7aa7-4077-8573-a02eea9d1dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training and validation of model_rf took 27.02 seconds across 102 iterations.\n",
      "\n",
      "Mean Accuracy:          0.61655868 ± 0.02484951\n",
      "Mean Weighted F1-Score: 0.58984693 ± 0.02445527\n",
      "Mean Micro F1-Score:    0.61655868 ± 0.02484951\n",
      "Mean Macro F1-Score:    0.29649956 ± 0.01488594\n",
      "\n",
      "Each fold had 539 entries for training and 269 for validation.\n"
     ]
    }
   ],
   "source": [
    "# Start timing\n",
    "start_time = datetime.now()\n",
    "\n",
    "# Initialize lists to store metrics\n",
    "accuracy_scores = []\n",
    "weighted_f1_scores = []\n",
    "micro_f1_scores = []\n",
    "macro_f1_scores = []\n",
    "fold = 1\n",
    "\n",
    "# Cross-validation loop\n",
    "for train_idx, val_idx in rskf.split(train_val_X, train_val_y):\n",
    "\n",
    "    # Uncomment the print statement for debugging only\n",
    "    #print(f\"Starting model training and validation on fold {fold}\")\n",
    "    \n",
    "    # Split data\n",
    "    fold_train_X, fold_val_X = train_val_X.iloc[train_idx], train_val_X.iloc[val_idx]\n",
    "    fold_train_y, fold_val_y = train_val_y.iloc[train_idx], train_val_y.iloc[val_idx]\n",
    "    \n",
    "    # Train the base model\n",
    "    model_rf.fit(fold_train_X, fold_train_y)\n",
    "    \n",
    "    # Predict on the test set\n",
    "    fold_pred_y = model_rf.predict(fold_val_X)\n",
    "    \n",
    "    # Compute metrics\n",
    "    accuracy_scores.append(accuracy_score(fold_val_y, fold_pred_y))\n",
    "    weighted_f1_scores.append(f1_score(fold_val_y, fold_pred_y, average='weighted'))\n",
    "    micro_f1_scores.append(f1_score(fold_val_y, fold_pred_y, average='micro'))\n",
    "    macro_f1_scores.append(f1_score(fold_val_y, fold_pred_y, average='macro'))\n",
    "    \n",
    "    fold += 1\n",
    "\n",
    "# End timing\n",
    "end_time = datetime.now()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "# Aggregate results\n",
    "print(f\"The training and validation of model_rf took {elapsed_time.total_seconds():.2f} seconds across {rskf.get_n_splits()} iterations.\\n\")\n",
    "\n",
    "print(f\"Mean Accuracy:          {np.mean(accuracy_scores):.8f} ± {np.std(accuracy_scores):.8f}\")\n",
    "print(f\"Mean Weighted F1-Score: {np.mean(weighted_f1_scores):.8f} ± {np.std(weighted_f1_scores):.8f}\")\n",
    "print(f\"Mean Micro F1-Score:    {np.mean(micro_f1_scores):.8f} ± {np.std(micro_f1_scores):.8f}\")\n",
    "print(f\"Mean Macro F1-Score:    {np.mean(macro_f1_scores):.8f} ± {np.std(macro_f1_scores):.8f}\\n\")\n",
    "\n",
    "print(f\"Each fold had {len(fold_train_X)} entries for training and {len(fold_val_X)} for validation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83857d02-f6ab-4213-a2b3-8e87ecfd41ea",
   "metadata": {},
   "source": [
    "**criterion = \"gini\" AND max_features = 0.12**\n",
    "\n",
    "**The training and validation of model_rf took 26.07 seconds across 102 iterations.**\n",
    "\n",
    "Mean Accuracy:          0.61655868 ± 0.02484951\n",
    "\n",
    "Mean Weighted F1-Score: 0.58984693 ± 0.02445527\n",
    "\n",
    "Mean Micro F1-Score:    0.61655868 ± 0.02484951\n",
    "\n",
    "Mean Macro F1-Score:    0.29649956 ± 0.01488594\n",
    "\n",
    "**Each fold had 539 entries for training and 269 for validation.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4580a79f-5ec4-4457-a320-2084792c1761",
   "metadata": {},
   "source": [
    "**criterion = \"gini\" AND max_features = 100**\n",
    "\n",
    "**The training and validation of model_rf took 26.04 seconds across 102 iterations.**\n",
    "\n",
    "Mean Accuracy:          0.61478026 ± 0.02414821\n",
    "\n",
    "Mean Weighted F1-Score: 0.58796849 ± 0.02367034\n",
    "\n",
    "Mean Micro F1-Score:    0.61478026 ± 0.02414821\n",
    "\n",
    "Mean Macro F1-Score:    0.29525023 ± 0.01380702\n",
    "\n",
    "**Each fold had 539 entries for training and 269 for validation.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686bca5b-6a70-4f6b-9205-912d386b4a3b",
   "metadata": {},
   "source": [
    "**criterion = \"gini\" AND max_features = \"sqrt\"**\n",
    "\n",
    "**The training and validation of model_rf took 32.69 seconds across 102 iterations.**\n",
    "\n",
    "Mean Accuracy:          0.60528086 ± 0.02410240\n",
    "\n",
    "Mean Weighted F1-Score: 0.57394776 ± 0.02473896\n",
    "\n",
    "Mean Micro F1-Score:    0.60528086 ± 0.02410240\n",
    "\n",
    "Mean Macro F1-Score:    0.29011765 ± 0.01563658\n",
    "\n",
    "**Each fold had 539 entries for training and 269 for validation.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d5e359-c588-4e1c-b000-be8d5d801725",
   "metadata": {},
   "source": [
    "**TEST SET RESULTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ff95d7d-a527-409a-9dc5-55a29e14d1c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model classified correctly 133 entries from a total of 203.\n",
      "\n",
      "Accuracy on test set:          0.6551724137931034\n",
      "Weighted F1-Score on test set: 0.628514051676953\n",
      "\n",
      "F1-Score per class\n",
      "\n",
      "Class 11: F1-Score = 0.000000\n",
      "Class 13: F1-Score = 0.786207\n",
      "Class 14: F1-Score = 0.666667\n",
      "Class 16: F1-Score = 0.000000\n",
      "Class 17: F1-Score = 0.000000\n",
      "Class 2: F1-Score = 0.840000\n",
      "Class 3: F1-Score = 0.000000\n",
      "Class 4: F1-Score = 0.444444\n",
      "Class 6: F1-Score = 0.000000\n",
      "Class 7: F1-Score = 0.000000\n",
      "Class 9: F1-Score = 0.521739\n",
      "Class 99: F1-Score = 0.514286\n",
      "Class macro avg: F1-Score = 0.314445\n",
      "Class weighted avg: F1-Score = 0.628514\n",
      "\n",
      "Accuracy per class\n",
      "\n",
      "Class 11: Recall = 0.000000\n",
      "Class 13: Recall = 0.890625\n",
      "Class 14: Recall = 0.633333\n",
      "Class 16: Recall = 0.000000\n",
      "Class 17: Recall = 0.000000\n",
      "Class 2: Recall = 0.875000\n",
      "Class 3: Recall = 0.000000\n",
      "Class 4: Recall = 0.375000\n",
      "Class 6: Recall = 0.000000\n",
      "Class 7: Recall = 0.000000\n",
      "Class 9: Recall = 0.521739\n",
      "Class 99: Recall = 0.514286\n",
      "Class macro avg: Recall = 0.317499\n",
      "Class weighted avg: Recall = 0.655172\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Train the base model\n",
    "model_rf.fit(train_val_X, train_val_y)\n",
    "\n",
    "# Predict on the test set\n",
    "test_pred_y = model_rf.predict(test_X)\n",
    "\n",
    "# Compute and display metrics\n",
    "print(f\"The model classified correctly {sum(test_y == test_pred_y)} entries from a total of {len(test_X)}.\\n\")\n",
    "\n",
    "print(f\"Accuracy on test set:          {accuracy_score(test_y, test_pred_y)}\")\n",
    "print(f\"Weighted F1-Score on test set: {f1_score(test_y, test_pred_y, average='weighted')}\\n\")\n",
    "\n",
    "print(\"F1-Score per class\\n\")\n",
    "\n",
    "# Generate classification report\n",
    "report = classification_report(test_y, test_pred_y, output_dict=True, zero_division=0)\n",
    "\n",
    "# Display F1-score per class\n",
    "for class_label, metrics in report.items():\n",
    "    if isinstance(metrics, dict) and 'f1-score' in metrics:\n",
    "        print(f\"Class {class_label}: F1-Score = {metrics['f1-score']:.6f}\")\n",
    "\n",
    "print(\"\\nAccuracy per class\\n\")\n",
    "\n",
    "# Display F1-score per class\n",
    "for class_label, metrics in report.items():\n",
    "    if isinstance(metrics, dict) and 'recall' in metrics:\n",
    "        print(f\"Class {class_label}: Recall = {metrics['recall']:.6f}\") # Recall is equivalent to per-class accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e1e184-0386-4cb7-99b1-8b81417b1d58",
   "metadata": {},
   "source": [
    "**criterion = \"gini\" AND max_features = 0.12**\n",
    "\n",
    "**The model classified correctly 133 entries from a total of 203.**\n",
    "\n",
    "Accuracy on test set:          0.6551724137931034\n",
    "\n",
    "Weighted F1-Score on test set: 0.628514051676953\n",
    "\n",
    "**F1-Score per class**\n",
    "\n",
    "Class 11: F1-Score = 0.000000\n",
    "\n",
    "Class 13: F1-Score = 0.786207\n",
    "\n",
    "Class 14: F1-Score = 0.666667\n",
    "\n",
    "Class 16: F1-Score = 0.000000\n",
    "\n",
    "Class 17: F1-Score = 0.000000\n",
    "\n",
    "Class 2: F1-Score = 0.840000\n",
    "\n",
    "Class 3: F1-Score = 0.000000\n",
    "\n",
    "Class 4: F1-Score = 0.444444\n",
    "\n",
    "Class 6: F1-Score = 0.000000\n",
    "\n",
    "Class 7: F1-Score = 0.000000\n",
    "\n",
    "Class 9: F1-Score = 0.521739\n",
    "\n",
    "Class 99: F1-Score = 0.514286\n",
    "\n",
    "Class macro avg: F1-Score = 0.314445\n",
    "\n",
    "Class weighted avg: F1-Score = 0.628514"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d97759-765c-4513-8598-05aac7970c97",
   "metadata": {},
   "source": [
    "**criterion = \"gini\" AND max_features = 100**\n",
    "\n",
    "**The model classified correctly 133 entries from a total of 203.**\n",
    "\n",
    "Accuracy on test set:          0.6551724137931034\n",
    "\n",
    "Weighted F1-Score on test set: 0.6252145851467866\n",
    "\n",
    "**F1-Score per class**\n",
    "\n",
    "Class 11: F1-Score = 0.000000\n",
    "\n",
    "Class 13: F1-Score = 0.794521\n",
    "\n",
    "Class 14: F1-Score = 0.592593\n",
    "\n",
    "Class 16: F1-Score = 0.000000\n",
    "\n",
    "Class 17: F1-Score = 0.000000\n",
    "\n",
    "Class 2: F1-Score = 0.862745\n",
    "\n",
    "Class 3: F1-Score = 0.000000\n",
    "\n",
    "Class 4: F1-Score = 0.416667\n",
    "\n",
    "Class 6: F1-Score = 0.000000\n",
    "\n",
    "Class 7: F1-Score = 0.000000\n",
    "\n",
    "Class 9: F1-Score = 0.521739\n",
    "\n",
    "Class 99: F1-Score = 0.540541\n",
    "\n",
    "Class macro avg: F1-Score = 0.310734\n",
    "\n",
    "Class weighted avg: F1-Score = 0.625215"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12b7a49-cd13-4b0d-90e6-836ef313c739",
   "metadata": {},
   "source": [
    "**criterion = \"gini\"**\n",
    "\n",
    "**The model classified correctly 129 entries from a total of 203.**\n",
    "\n",
    "Accuracy on test set:          0.6354679802955665\n",
    "\n",
    "Weighted F1-Score on test set: 0.6082172179276506\n",
    "\n",
    "**F1-Score per class**\n",
    "\n",
    "Class 11: F1-Score = 0.333333\n",
    "\n",
    "Class 13: F1-Score = 0.748387\n",
    "\n",
    "Class 14: F1-Score = 0.653846\n",
    "\n",
    "Class 16: F1-Score = 0.000000\n",
    "\n",
    "Class 17: F1-Score = 0.000000\n",
    "\n",
    "Class 2: F1-Score = 0.862745\n",
    "\n",
    "Class 3: F1-Score = 0.000000\n",
    "\n",
    "Class 4: F1-Score = 0.434783\n",
    "\n",
    "Class 6: F1-Score = 0.000000\n",
    "\n",
    "Class 7: F1-Score = 0.000000\n",
    "\n",
    "Class 9: F1-Score = 0.521739\n",
    "\n",
    "Class 99: F1-Score = 0.417910\n",
    "\n",
    "Class macro avg: F1-Score = 0.331062\n",
    "\n",
    "Class weighted avg: F1-Score = 0.608217"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d61058-0279-4d18-8485-a1ca30d91335",
   "metadata": {},
   "source": [
    "**SAVE AND EXPORT RESULTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5f33ec4-8ea1-4018-89c2-b21dafd660c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Create DataFrame for Scores\n",
    "accuracy_rf_params_df = pd.DataFrame({'score': accuracy_scores})\n",
    "weighted_f1_rf_params_df = pd.DataFrame({'score': weighted_f1_scores})\n",
    "micro_f1_rf_params_df = pd.DataFrame({'score': micro_f1_scores})\n",
    "macro_f1_rf_params_df = pd.DataFrame({'score': macro_f1_scores})\n",
    "\n",
    "# Export the DataFrame to a CSV file\n",
    "accuracy_rf_params_df .to_csv('accuracy_rf_params.csv', index=False)\n",
    "weighted_f1_rf_params_df.to_csv('weighted_f1_rf_params.csv', index=False)\n",
    "micro_f1_rf_params_df.to_csv('micro_f1_rf_params.csv', index=False)\n",
    "macro_f1_rf_params_df.to_csv('macro_f1_rf_params.csv', index=False)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

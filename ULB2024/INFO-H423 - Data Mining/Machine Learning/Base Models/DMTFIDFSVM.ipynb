{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9708b2f9-ec8b-4e65-8658-c88f43ae4ec4",
   "metadata": {},
   "source": [
    "**SUPPORT VECTOR MACHINES (SVM)**\n",
    "\n",
    "**DATA LOADING AND PREPARATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31d1dece-7ab4-460f-b147-96d4e170b293",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e621fda5-0943-4a16-bf4b-ef943cf6de6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1011 entries, 0 to 1010\n",
      "Columns: 828 entries, incident_id to 998\n",
      "dtypes: float64(826), int64(1), string(1)\n",
      "memory usage: 6.4 MB\n"
     ]
    }
   ],
   "source": [
    "# Read the CSV\n",
    "tfidf_df = pd.read_csv(\"tfidf_sncb.csv\", sep='\\,', engine='python')\n",
    "\n",
    "tfidf_df['incident_type'] = tfidf_df['incident_type'].astype('string') \n",
    "\n",
    "tfidf_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db029af2-2198-4aa1-b5a4-65741c9d6515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train_val_X pandas df has 808 rows and 826 columns.\n",
      "The test_y pandas series has 203 rows and 1 column.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Filter in the Features (the values acquired from the events sequence after TF-IDF)\n",
    "X = tfidf_df.drop(['incident_type', 'incident_id'], axis=1) \n",
    "\n",
    "# Filter in the Target variable (labels / incident types)\n",
    "y = tfidf_df['incident_type']  \n",
    "\n",
    "# setting random_state constant to be used in the whole pipeline and guarantee reproducibility\n",
    "r_state = 123\n",
    "\n",
    "# Split data into training+validation and testing sets\n",
    "train_val_X, test_X, train_val_y, test_y = train_test_split(X, \n",
    "                                                            y, \n",
    "                                                            train_size = 0.8, \n",
    "                                                            random_state = r_state, # setting random_state for reproducibility\n",
    "                                                            stratify = y) # to respect class imbalance in the label column\n",
    "\n",
    "print(f\"The train_val_X pandas df has {len(train_val_X)} rows and {len(train_val_X.columns)} columns.\")\n",
    "print(f\"The test_y pandas series has {len(test_y)} rows and 1 column.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6553064-17ab-4507-841d-28d033d59e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In RepeatedStratifiedKFold() function, the parameter n_splits has to be set atmost to 3, due to class imbalance in the label column.\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# get the size of the smallest incident type class\n",
    "value_counts = Counter(train_val_y)\n",
    "min_class_setsize = min(value_counts.values())\n",
    "\n",
    "print(f\"In RepeatedStratifiedKFold() function, the parameter n_splits has to be set atmost to {min_class_setsize}, due to class imbalance in the label column.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba71c940-735d-4b1f-b59c-ad73e542bbbb",
   "metadata": {},
   "source": [
    "**MODEL TRAINING AND VALIDATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc889ca3-7aa7-4077-8573-a02eea9d1dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training of model_svm took 21.63 seconds across 102 iterations.\n",
      "\n",
      "Mean Accuracy:          0.38985295 ± 0.01473931\n",
      "Mean Weighted F1-Score: 0.29115318 ± 0.01685112\n",
      "Mean Micro F1-Score:    0.38985295 ± 0.01473931\n",
      "Mean Macro F1-Score:    0.11472494 ± 0.00912497\n",
      "\n",
      "Each fold had 539 entries for training and 269 for validation.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "# Define the base model\n",
    "model_svm = SVC()\n",
    "\n",
    "# Set up cross-validation\n",
    "rskf = RepeatedStratifiedKFold(n_splits=3, n_repeats=34, random_state=r_state)\n",
    "\n",
    "# Start timing\n",
    "start_time = datetime.now()\n",
    "\n",
    "# Initialize lists to store metrics\n",
    "accuracy_scores = []\n",
    "weighted_f1_scores = []\n",
    "micro_f1_scores = []\n",
    "macro_f1_scores = []\n",
    "fold = 1\n",
    "\n",
    "# Cross-validation loop\n",
    "for train_idx, val_idx in rskf.split(train_val_X, train_val_y):\n",
    "\n",
    "    # Uncomment the print statement for debugging only\n",
    "    #print(f\"Starting model training and validation on fold {fold}\")\n",
    "    \n",
    "    # Split data\n",
    "    fold_train_X, fold_val_X = train_val_X.iloc[train_idx], train_val_X.iloc[val_idx]\n",
    "    fold_train_y, fold_val_y = train_val_y.iloc[train_idx], train_val_y.iloc[val_idx]\n",
    "    \n",
    "    # Train the base model\n",
    "    model_svm.fit(fold_train_X, fold_train_y)\n",
    "    \n",
    "    # Predict on the test set\n",
    "    fold_pred_y = model_svm.predict(fold_val_X)\n",
    "    \n",
    "    # Compute metrics\n",
    "    accuracy_scores.append(accuracy_score(fold_val_y, fold_pred_y))\n",
    "    weighted_f1_scores.append(f1_score(fold_val_y, fold_pred_y, average='weighted'))\n",
    "    micro_f1_scores.append(f1_score(fold_val_y, fold_pred_y, average='micro'))\n",
    "    macro_f1_scores.append(f1_score(fold_val_y, fold_pred_y, average='macro'))\n",
    "    \n",
    "    fold += 1\n",
    "\n",
    "# End timing\n",
    "end_time = datetime.now()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "# Aggregate results\n",
    "print(f\"The training of model_svm took {elapsed_time.total_seconds():.2f} seconds across {rskf.get_n_splits()} iterations.\\n\")\n",
    "\n",
    "print(f\"Mean Accuracy:          {np.mean(accuracy_scores):.8f} ± {np.std(accuracy_scores):.8f}\")\n",
    "print(f\"Mean Weighted F1-Score: {np.mean(weighted_f1_scores):.8f} ± {np.std(weighted_f1_scores):.8f}\")\n",
    "print(f\"Mean Micro F1-Score:    {np.mean(micro_f1_scores):.8f} ± {np.std(micro_f1_scores):.8f}\")\n",
    "print(f\"Mean Macro F1-Score:    {np.mean(macro_f1_scores):.8f} ± {np.std(macro_f1_scores):.8f}\\n\")\n",
    "\n",
    "print(f\"Each fold had {len(fold_train_X)} entries for training and {len(fold_val_X)} for validation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcdd176-3dcb-4a83-9dfc-fb4c6effe0d5",
   "metadata": {},
   "source": [
    "**The training of model_svm took 21.63 seconds across 102 iterations.**\n",
    "\n",
    "Mean Accuracy:          0.38985295 ± 0.01473931\n",
    "\n",
    "Mean Weighted F1-Score: 0.29115318 ± 0.01685112\n",
    "\n",
    "Mean Micro F1-Score:    0.38985295 ± 0.01473931\n",
    "\n",
    "Mean Macro F1-Score:    0.11472494 ± 0.00912497\n",
    "\n",
    "**Each fold had 539 entries for training and 269 for validation.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd53dd76-c2be-478b-a83f-3b5424a27187",
   "metadata": {},
   "source": [
    "**TEST SET RESULTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4cdcaa47-272a-49b9-a20d-1aa2ddbbb9d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model classified correctly 86 entries from a total of 203.\n",
      "\n",
      "Accuracy on test set:          0.4236453201970443\n",
      "Weighted F1-Score on test set: 0.3622347409448813\n",
      "\n",
      "F1-Score per class\n",
      "\n",
      "Class 11: F1-Score = 0.000000\n",
      "Class 13: F1-Score = 0.517544\n",
      "Class 14: F1-Score = 0.439024\n",
      "Class 16: F1-Score = 0.000000\n",
      "Class 17: F1-Score = 0.000000\n",
      "Class 2: F1-Score = 0.275862\n",
      "Class 3: F1-Score = 0.000000\n",
      "Class 4: F1-Score = 0.315789\n",
      "Class 6: F1-Score = 0.000000\n",
      "Class 7: F1-Score = 0.000000\n",
      "Class 9: F1-Score = 0.160000\n",
      "Class 99: F1-Score = 0.339623\n",
      "Class macro avg: F1-Score = 0.170654\n",
      "Class weighted avg: F1-Score = 0.362235\n",
      "\n",
      "Accuracy per class\n",
      "\n",
      "Class 11: Recall = 0.000000\n",
      "Class 13: Recall = 0.921875\n",
      "Class 14: Recall = 0.300000\n",
      "Class 16: Recall = 0.000000\n",
      "Class 17: Recall = 0.000000\n",
      "Class 2: Recall = 0.166667\n",
      "Class 3: Recall = 0.000000\n",
      "Class 4: Recall = 0.187500\n",
      "Class 6: Recall = 0.000000\n",
      "Class 7: Recall = 0.000000\n",
      "Class 9: Recall = 0.086957\n",
      "Class 99: Recall = 0.257143\n",
      "Class macro avg: Recall = 0.160012\n",
      "Class weighted avg: Recall = 0.423645\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Train the base model\n",
    "model_svm.fit(train_val_X, train_val_y)\n",
    "\n",
    "# Predict on the test set\n",
    "test_pred_y = model_svm.predict(test_X)\n",
    "\n",
    "# Compute and display metrics\n",
    "print(f\"The model classified correctly {sum(test_y == test_pred_y)} entries from a total of {len(test_X)}.\\n\")\n",
    "\n",
    "print(f\"Accuracy on test set:          {accuracy_score(test_y, test_pred_y)}\")\n",
    "print(f\"Weighted F1-Score on test set: {f1_score(test_y, test_pred_y, average='weighted')}\\n\")\n",
    "\n",
    "print(\"F1-Score per class\\n\")\n",
    "\n",
    "# Generate classification report\n",
    "report = classification_report(test_y, test_pred_y, output_dict=True, zero_division=0)\n",
    "\n",
    "# Display F1-score per class\n",
    "for class_label, metrics in report.items():\n",
    "    if isinstance(metrics, dict) and 'f1-score' in metrics:\n",
    "        print(f\"Class {class_label}: F1-Score = {metrics['f1-score']:.6f}\")\n",
    "\n",
    "print(\"\\nAccuracy per class\\n\")\n",
    "\n",
    "# Display F1-score per class\n",
    "for class_label, metrics in report.items():\n",
    "    if isinstance(metrics, dict) and 'recall' in metrics:\n",
    "        print(f\"Class {class_label}: Recall = {metrics['recall']:.6f}\") # Recall is equivalent to per-class accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a846c99-9059-4edc-8df5-1d416e046474",
   "metadata": {},
   "source": [
    "**The model classified correctly 86 entries from a total of 203.**\n",
    "\n",
    "Accuracy on test set:          0.4236453201970443\n",
    "\n",
    "Weighted F1-Score on test set: 0.3622347409448813\n",
    "\n",
    "**F1-Score per class**\n",
    "\n",
    "Class 11: F1-Score = 0.000000\n",
    "\n",
    "Class 13: F1-Score = 0.517544\n",
    "\n",
    "Class 14: F1-Score = 0.439024\n",
    "\n",
    "Class 16: F1-Score = 0.000000\n",
    "\n",
    "Class 17: F1-Score = 0.000000\n",
    "\n",
    "Class 2: F1-Score = 0.275862\n",
    "\n",
    "Class 3: F1-Score = 0.000000\n",
    "\n",
    "Class 4: F1-Score = 0.315789\n",
    "\n",
    "Class 6: F1-Score = 0.000000\n",
    "\n",
    "Class 7: F1-Score = 0.000000\n",
    "\n",
    "Class 9: F1-Score = 0.160000\n",
    "\n",
    "Class 99: F1-Score = 0.339623\n",
    "\n",
    "Class macro avg: F1-Score = 0.170654\n",
    "\n",
    "Class weighted avg: F1-Score = 0.362235"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76841fa9-c0fc-46b7-a50a-4b747594c9fb",
   "metadata": {},
   "source": [
    "**SAVE AND EXPORT RESULTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5f33ec4-8ea1-4018-89c2-b21dafd660c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Create DataFrame for Scores\n",
    "accuracy_svm_df = pd.DataFrame({'score': accuracy_scores})\n",
    "weighted_f1_svm_df = pd.DataFrame({'score': weighted_f1_scores})\n",
    "micro_f1_svm_df = pd.DataFrame({'score': micro_f1_scores})\n",
    "macro_f1_svm_df = pd.DataFrame({'score': macro_f1_scores})\n",
    "\n",
    "# Export the DataFrame to a CSV file\n",
    "accuracy_svm_df.to_csv('accuracy_svm.csv', index=False)\n",
    "weighted_f1_svm_df.to_csv('weighted_f1_svm.csv', index=False)\n",
    "micro_f1_svm_df.to_csv('micro_f1_svm.csv', index=False)\n",
    "macro_f1_svm_df.to_csv('macro_f1_svm.csv', index=False)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "567633ff-8fce-4293-b07a-1821102cd79e",
   "metadata": {},
   "source": [
    "**NAIVE BAYES (NB)**\n",
    "\n",
    "**DATA LOADING AND PREPARATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31d1dece-7ab4-460f-b147-96d4e170b293",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e621fda5-0943-4a16-bf4b-ef943cf6de6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1011 entries, 0 to 1010\n",
      "Columns: 828 entries, incident_id to 998\n",
      "dtypes: float64(826), int64(1), string(1)\n",
      "memory usage: 6.4 MB\n"
     ]
    }
   ],
   "source": [
    "# Read the CSV\n",
    "tfidf_df = pd.read_csv(\"tfidf_sncb.csv\", sep='\\,', engine='python')\n",
    "\n",
    "tfidf_df['incident_type'] = tfidf_df['incident_type'].astype('string') \n",
    "\n",
    "tfidf_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db029af2-2198-4aa1-b5a4-65741c9d6515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train_val_X pandas df has 808 rows and 826 columns.\n",
      "The test_y pandas series has 203 rows and 1 column.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Filter in the Features (the values acquired from the events sequence after TF-IDF)\n",
    "X = tfidf_df.drop(['incident_type', 'incident_id'], axis=1) \n",
    "\n",
    "# Filter in the Target variable (labels / incident types)\n",
    "y = tfidf_df['incident_type']  \n",
    "\n",
    "# setting random_state constant to be used in the whole pipeline and guarantee reproducibility\n",
    "r_state = 123\n",
    "\n",
    "# Split data into training+validation and testing sets\n",
    "train_val_X, test_X, train_val_y, test_y = train_test_split(X, \n",
    "                                                            y, \n",
    "                                                            train_size = 0.8, \n",
    "                                                            random_state = r_state, # setting random_state for reproducibility\n",
    "                                                            stratify = y) # to respect class imbalance in the label column\n",
    "\n",
    "print(f\"The train_val_X pandas df has {len(train_val_X)} rows and {len(train_val_X.columns)} columns.\")\n",
    "print(f\"The test_y pandas series has {len(test_y)} rows and 1 column.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6553064-17ab-4507-841d-28d033d59e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In RepeatedStratifiedKFold() function, the parameter n_splits has to be set atmost to 3, due to class imbalance in the label column.\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# get the size of the smallest incident type class\n",
    "value_counts = Counter(train_val_y)\n",
    "min_class_setsize = min(value_counts.values())\n",
    "\n",
    "print(f\"In RepeatedStratifiedKFold() function, the parameter n_splits has to be set atmost to {min_class_setsize}, due to class imbalance in the label column.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba71c940-735d-4b1f-b59c-ad73e542bbbb",
   "metadata": {},
   "source": [
    "**MODEL TRAINING AND VALIDATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc889ca3-7aa7-4077-8573-a02eea9d1dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training of model_gnb took 3.78 seconds across 102 iterations.\n",
      "\n",
      "Mean Accuracy:          0.47590595 ± 0.02432288\n",
      "Mean Weighted F1-Score: 0.48313171 ± 0.02451889\n",
      "Mean Micro F1-Score:    0.47590595 ± 0.02432288\n",
      "Mean Macro F1-Score:    0.28317088 ± 0.01868841\n",
      "\n",
      "Each fold had 539 entries for training and 269 for validation.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "#from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "# Define the base model\n",
    "model_gnb = GaussianNB()\n",
    "\n",
    "# Set up cross-validation\n",
    "rskf = RepeatedStratifiedKFold(n_splits = min_class_setsize, \n",
    "                               n_repeats = 34, \n",
    "                               random_state = r_state)\n",
    "\n",
    "# Start timing\n",
    "start_time = datetime.now()\n",
    "\n",
    "# Initialize lists to store metrics\n",
    "accuracy_scores = []\n",
    "weighted_f1_scores = []\n",
    "micro_f1_scores = []\n",
    "macro_f1_scores = []\n",
    "fold = 1\n",
    "\n",
    "# Cross-validation loop\n",
    "for train_idx, val_idx in rskf.split(train_val_X, train_val_y):\n",
    "\n",
    "    # Uncomment the print statement for debugging only\n",
    "    #print(f\"Starting model training and validation on fold {fold}\")\n",
    "    \n",
    "    # Split data\n",
    "    fold_train_X, fold_val_X = train_val_X.iloc[train_idx], train_val_X.iloc[val_idx]\n",
    "    fold_train_y, fold_val_y = train_val_y.iloc[train_idx], train_val_y.iloc[val_idx]\n",
    "    \n",
    "    # Train the base model\n",
    "    model_gnb.fit(fold_train_X, fold_train_y)\n",
    "    \n",
    "    # Predict on the test set\n",
    "    fold_pred_y = model_gnb.predict(fold_val_X)\n",
    "    \n",
    "    # Compute metrics\n",
    "    accuracy_scores.append(accuracy_score(fold_val_y, fold_pred_y))\n",
    "    weighted_f1_scores.append(f1_score(fold_val_y, fold_pred_y, average='weighted'))\n",
    "    micro_f1_scores.append(f1_score(fold_val_y, fold_pred_y, average='micro'))\n",
    "    macro_f1_scores.append(f1_score(fold_val_y, fold_pred_y, average='macro'))\n",
    "    \n",
    "    fold += 1\n",
    "\n",
    "# End timing\n",
    "end_time = datetime.now()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "# Aggregate results\n",
    "print(f\"The training of model_gnb took {elapsed_time.total_seconds():.2f} seconds across {rskf.get_n_splits()} iterations.\\n\")\n",
    "\n",
    "print(f\"Mean Accuracy:          {np.mean(accuracy_scores):.8f} ± {np.std(accuracy_scores):.8f}\")\n",
    "print(f\"Mean Weighted F1-Score: {np.mean(weighted_f1_scores):.8f} ± {np.std(weighted_f1_scores):.8f}\")\n",
    "print(f\"Mean Micro F1-Score:    {np.mean(micro_f1_scores):.8f} ± {np.std(micro_f1_scores):.8f}\")\n",
    "print(f\"Mean Macro F1-Score:    {np.mean(macro_f1_scores):.8f} ± {np.std(macro_f1_scores):.8f}\\n\")\n",
    "\n",
    "print(f\"Each fold had {len(fold_train_X)} entries for training and {len(fold_val_X)} for validation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3373dcf7-1e74-4f1e-b6d7-386fe9f96798",
   "metadata": {},
   "source": [
    "**The training of model_gnb took 3.78 seconds across 102 iterations.**\n",
    "\n",
    "Mean Accuracy:          0.47590595 ± 0.02432288\n",
    "    \n",
    "Mean Weighted F1-Score: 0.48313171 ± 0.02451889\n",
    "    \n",
    "Mean Micro F1-Score:    0.47590595 ± 0.02432288\n",
    "\n",
    "Mean Macro F1-Score:    0.28317088 ± 0.01868841\n",
    "\n",
    "**Each fold had 539 entries for training and 269 for validation.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3991a839-3d89-4673-b353-cce267cc3a5b",
   "metadata": {},
   "source": [
    "**TEST SET RESULTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56638c13-7114-4a4c-b745-8909698a5dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model classified correctly 88 entries from a total of 203.\n",
      "\n",
      "Accuracy on test set:          0.43349753694581283\n",
      "Weighted F1-Score on test set: 0.43675471430075485\n",
      "\n",
      "F1-Score per class\n",
      "\n",
      "Class 11: F1-Score = 0.181818\n",
      "Class 13: F1-Score = 0.475248\n",
      "Class 14: F1-Score = 0.518519\n",
      "Class 16: F1-Score = 0.000000\n",
      "Class 17: F1-Score = 0.000000\n",
      "Class 2: F1-Score = 0.423529\n",
      "Class 3: F1-Score = 0.000000\n",
      "Class 4: F1-Score = 0.411765\n",
      "Class 6: F1-Score = 0.000000\n",
      "Class 7: F1-Score = 0.000000\n",
      "Class 9: F1-Score = 0.750000\n",
      "Class 99: F1-Score = 0.222222\n",
      "Class macro avg: F1-Score = 0.248592\n",
      "Class weighted avg: F1-Score = 0.436755\n",
      "\n",
      "Accuracy per class\n",
      "\n",
      "Class 11: Recall = 0.200000\n",
      "Class 13: Recall = 0.375000\n",
      "Class 14: Recall = 0.466667\n",
      "Class 16: Recall = 0.000000\n",
      "Class 17: Recall = 0.000000\n",
      "Class 2: Recall = 0.750000\n",
      "Class 3: Recall = 0.000000\n",
      "Class 4: Recall = 0.437500\n",
      "Class 6: Recall = 0.000000\n",
      "Class 7: Recall = 0.000000\n",
      "Class 9: Recall = 0.782609\n",
      "Class 99: Recall = 0.171429\n",
      "Class macro avg: Recall = 0.265267\n",
      "Class weighted avg: Recall = 0.433498\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Train the base model\n",
    "model_gnb.fit(train_val_X, train_val_y)\n",
    "\n",
    "# Predict on the test set\n",
    "test_pred_y = model_gnb.predict(test_X)\n",
    "\n",
    "# Compute and display metrics\n",
    "print(f\"The model classified correctly {sum(test_y == test_pred_y)} entries from a total of {len(test_X)}.\\n\")\n",
    "\n",
    "print(f\"Accuracy on test set:          {accuracy_score(test_y, test_pred_y)}\")\n",
    "print(f\"Weighted F1-Score on test set: {f1_score(test_y, test_pred_y, average='weighted')}\\n\")\n",
    "\n",
    "print(\"F1-Score per class\\n\")\n",
    "\n",
    "# Generate classification report\n",
    "report = classification_report(test_y, test_pred_y, output_dict=True, zero_division=0)\n",
    "\n",
    "# Display F1-score per class\n",
    "for class_label, metrics in report.items():\n",
    "    if isinstance(metrics, dict) and 'f1-score' in metrics:\n",
    "        print(f\"Class {class_label}: F1-Score = {metrics['f1-score']:.6f}\")\n",
    "\n",
    "print(\"\\nAccuracy per class\\n\")\n",
    "\n",
    "# Display F1-score per class\n",
    "for class_label, metrics in report.items():\n",
    "    if isinstance(metrics, dict) and 'recall' in metrics:\n",
    "        print(f\"Class {class_label}: Recall = {metrics['recall']:.6f}\") # Recall is equivalent to per-class accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff974b5a-04de-4280-b4f0-55a480dc9acd",
   "metadata": {},
   "source": [
    "**The model classified correctly 88 entries from a total of 203.**\n",
    "\n",
    "Accuracy on test set:          0.43349753694581283\n",
    "\n",
    "Weighted F1-Score on test set: 0.43675471430075485\n",
    "\n",
    "**F1-Score per class**\n",
    "\n",
    "Class 11: F1-Score = 0.181818\n",
    "\n",
    "Class 13: F1-Score = 0.475248\n",
    "\n",
    "Class 14: F1-Score = 0.518519\n",
    "\n",
    "Class 16: F1-Score = 0.000000\n",
    "\n",
    "Class 17: F1-Score = 0.000000\n",
    "\n",
    "Class 2: F1-Score = 0.423529\n",
    "\n",
    "Class 3: F1-Score = 0.000000\n",
    "\n",
    "Class 4: F1-Score = 0.411765\n",
    "\n",
    "Class 6: F1-Score = 0.000000\n",
    "\n",
    "Class 7: F1-Score = 0.000000\n",
    "\n",
    "Class 9: F1-Score = 0.750000\n",
    "\n",
    "Class 99: F1-Score = 0.222222\n",
    "\n",
    "Class macro avg: F1-Score = 0.248592\n",
    "\n",
    "Class weighted avg: F1-Score = 0.436755"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d62cc85-e45f-4f39-be8c-57a106b9459d",
   "metadata": {},
   "source": [
    "**SAVE AND EXPORT RESULTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5f33ec4-8ea1-4018-89c2-b21dafd660c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Create DataFrame for Scores\n",
    "accuracy_gnb_df = pd.DataFrame({'score': accuracy_scores})\n",
    "weighted_f1_gnb_df = pd.DataFrame({'score': weighted_f1_scores})\n",
    "micro_f1_gnb_df = pd.DataFrame({'score': micro_f1_scores})\n",
    "macro_f1_gnb_df = pd.DataFrame({'score': macro_f1_scores})\n",
    "\n",
    "# Export the DataFrame to a CSV file\n",
    "accuracy_gnb_df .to_csv('accuracy_gnb.csv', index=False)\n",
    "weighted_f1_gnb_df.to_csv('weighted_f1_gnb.csv', index=False)\n",
    "micro_f1_gnb_df.to_csv('micro_f1_gnb.csv', index=False)\n",
    "macro_f1_gnb_df.to_csv('macro_f1_gnb.csv', index=False)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
